

<!DOCTYPE html>
<html>
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width">
      <title>Doubt Clearing</title>
      <script type="text/javascript" async
         src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>
      <link href="https://fonts.googleapis.com/css?family=Lato|Montserrat|Zilla+Slab&display=swap" rel="stylesheet">
      <style>
         /*Webdesign URL: https://www.quackit.com/html/tutorial/html_layouts.cfm*/
         body { 
         display: grid;
         grid-template-areas: 
         "header header header"
         "nav article ads"
         "footer footer footer";
         grid-template-rows: 120px 1fr 20px;
         grid-template-columns: 20% 1fr 5%;
         grid-gap: 20px;
         margin: 0;
         font-family: 'Zilla Slab', serif;
         font-family: 'Lato', sans-serif;
         /*font-family: 'Montserrat', sans-serif;*/
         }
         /* Stack the layout on small devices/viewports. */
         @media all and (max-width: 800px) {
         body { 
         grid-template-areas: 
         "header"
         "nav"
         "article"
         "ads"
         "footer";
         grid-template-rows: 120px 1fr 70px 1fr 70px;  
         grid-template-columns: 1fr;
         }
         }
         #pageHeader {
         grid-area: header;
         }
         #mainArticle { 
         grid-area: article;      
         }
         #mainNav { 
         grid-area: nav; 
         }
         #siteAds { 
         grid-area: ads; 
         }
         header, article, nav, div {
         padding: 5px;
         background: white;
         }
         /* Use a media query to add a breakpoint at 600px: */
         .highlightme { background-color:#f9e79f; font-family: Courier;} 
      </style>
   <body>
      <header id="pageHeader">
         <h1>Quick Refresher</h1>
         <a href="https://msank00.github.io/blog/">Back to Blog Content</a>
      </header>
      <article id="mainArticle">
         <h2 id="linearregression">Linear Regression</h2>
         <ul>
            <li>It's linear with respect to weight <span class="highlightme"><span class="highlightme"><code>w</code></span></span>, but not with respect to 
               input <span class="highlightme"><span class="highlightme"><code>x</code></span></span>
            </li>
            <li><span class="highlightme"><span class="highlightme"><code>posterior ~ liekelihood * prior</code></span></span></li>
            <li>Ordinary Least Square (OLS) approach to find the model parameters is a special case of maximum likelihood estimation and the overfitting 
               problem is a general property of the MLE. But by adopting the Bayesian approach, the overfitting problem can be avoided. 
               <span class="highlightme"><code>[p9, Bishop]</code></span>
            </li>
            <li>Also from Bayesian perspective we can use model for which number of parameters can exceed the number of training 
               data. In Bayesian learning, the effective number of parameters adapts automatically to the size of the data.
            </li>
         </ul>
         <p><strong>Point Estimate of W vs W Distribution</strong></p>
         <p>Consider D is our dataset and w is the parameter set. Now in both Bayesian and frequentist paradigm, 
            the likelihood function <span class="highlightme"><code>p(D|w)</code></span> plays a central role. In frequentist approach, w is considered to be
            fixed parameter, whose value is determined by some form of estimator and the error bars on the estimator are obtained by considering the distribution 
            of possible data sets D. 
         </p>
         <p>However, in Bayesian setting we have only one sigle datasets D (the observed datasets), and the uncertainty in parameters is expressed through a 
            probability distribution over <span class="highlightme"><code>w</code></span>. <span class="highlightme"><code>[p22-p23]</code></span>
         </p>
         <p>A widely used frequentist estimator is the maximul likelihood, in which w is set to the value that maximizes <span class="highlightme"><code>p(D|w)</code></span>. 
            In the ML literature, the negative log of the likelihood is considered as _error function_. As the negative log is a monotonically decreasing function, 
            so maximizing the likelihood is equivalent to minimizing the error.
         </p>
         <p>However Bayesian approach has a very common criticism, i.e. the inclusion of prior belief. Therefore, people try to use _noninformative prior_ to 
            get rid of the prior dependences. <span class="highlightme"><code>[p23]</code></span>.
         </p>
         <p><strong>Criticism for MLE</strong></p>
         <ul>
            <li>MLE suffers from <span class="highlightme"><code>overfitting</code></span> problem. In general overfitting means the model fitted to the training 
               data so perfectly that if we slightly change the data and get prediction, test error will be very high. 
               That means the model is sensitive to the variance of data. The theoretical reason is MLE systematically undermines the variance of the 
               distribution. See proof <span class="highlightme"><code>[[p27], [figure 1.15,p26, p28]]</code></span>.  
               Because here <span class="highlightme"><code>sample variance</code></span> is measured using the <span class="highlightme"><code>sample mean</code></span>, 
               instead of the population mean.
            </li>
            <li>Sample mean is an unbiased estimator of the population mean, but sample variance is a biased estimator of the population variance. 
               <span class="highlightme"><code>[p27], [figure 1.15,p26]</code></span>
            </li>
            <li>If you see the image <span class="highlightme"><code>p28</code></span>, the 3 red curve shows 3 different dataset and the 
               green curve shows the true dataset. And the mean of the 3 red curves coincide with the mean of the green curve, 
               but their variances are different. 
            </li>
         </ul>
         <p>In Bayesian curve fitting setting, the sum of squared error function has arisen as a consequence of maximizing the likelihood under the assumption of Gaussian noise distribution.</p>
         <p>Regularization allows complex models to be trained on the datasets of limited size without severe overfitting, essentially by limiting the model complexity. <span class="highlightme"><code>[p145, bishop]</code></span></p>
         <p><strong>Bias Variance Trade-off from Frequentist viewpoint</strong></p>
         <p>In frequentist viewpoint, <span class="highlightme"><code>w</code></span> is fixed and error bars on the estimators are obtained by considering the distribution over the data D. <span class="highlightme"><code>[p22-23; Bishop]</code></span>.</p>
         <p>Suppose we have large number of <strong>data sets</strong>, <span class="highlightme"><code>[D1,...,Dn]</code></span>, each of size N and each drawn independently from distribution of <span class="highlightme"><code>p(t,x)</code></span>. For any given data set <span class="highlightme"><code>Di</code></span> we can run our learning algorithm and get a prediction function <span class="highlightme"><code>y(x;Di)</code></span>. Different datasets from the ensemble will give different prediction functions and consequently different values of squared loss. The performance of a particular learning algorithm is then assessed by averaging over this ensemble of datasets. <span class="highlightme"><code>[p148; Bishop]</code></span>. </p>
         <p>Our original regression function is <span class="highlightme"><code>Y</code></span> and say for <span class="highlightme"><code>Di</code></span> 
            we got our predictive function \(\hat{Y_i}\).
         </p>
         <p><strong>Bias</strong> = \((E[\hat{Y_i}(x;D_i)] - Y)^2\), where \(E[\hat{Y_i}(x;D_i)]\) is average (expected) performance over all the datasets. 
            So, Bias represents the extent to which the average prediction over all the datasets \(D_i\) differ from the desired regression function \(Y\).
         </p>
         <p><strong>Variance</strong> = \(E[(\hat{Y_i}(x;D_i) - E[\hat{Y_i}(x;D_i)])^2]\), where \(\hat{Y_i}(x;D_i)\) is the predictive 
            function over data set \(D_i\) and \(E[\hat{Y_i}(x;D_i)]\) is the average performance over all the datasets. 
            So variance represents, the extent to which the individual predictive functions \(\hat{Y_i}\) for dataset 
            \(D_i\) varies around their average. And thus we measure the extent by which the function \(Y(x;D)\) is sensitive to the particular 
            choice of the data sets. <span class="highlightme"><code>[p149; Bishop]</code></span>
         </p>
         <a href="#top">Back to top of page</a>
         <hr />
         <h2 id="linearmodels">Linear Models</h2>
         <h3 id="generallinearmodel">General Linear Model</h3>
         <p>Indeed, the general linear model can be seen as an extension of linear multiple regression for a single dependent variable. 
            Understanding the multiple regression model is fundamental to understand the general linear model.
         </p>
         <h4 id="singleregression">Single Regression</h4>
         <p>One independent variable and one dependent variable</p>
         <p>$$y=\theta_0+\theta x_1$$</p>
         <h4 id="multipleregression">Multiple Regression</h4>
         <p>Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).</p>
         <p><strong>TL;DR:</strong> Multiple linear regression is the most common form of linear regression analysis.  As a predictive analysis, the multiple linear regression is used to explain the relationship between one continuous dependent variable and two or more independent variables</p>
         <p>$$y=\theta_0+\theta x_1+\theta x_2+...+\theta x_n$$</p>
         <h4 id="additivemodel">Additive Model:</h4>
         <p>$$y=\theta_0+\theta f_1(x_1)+\theta f_2(x_2)+...+\theta f_n(x_n)$$</p>
         <p>A generalization of the multiple regression model would be to maintain the additive nature of the model, but to replace the simple terms of the linear 
            equation \(\theta_i * x_i\) with \(f_i(x_i)\) where \(f_i()\) is a non-parametric function of the predictor \(x_i\).  
            In other words, instead of a single coefficient for each variable (additive term) in the model, 
            in additive models an unspecified (non-parametric) function is estimated for each predictor, to achieve the best prediction of the dependent variable values.
         </p>
         <p><strong>General Linear Model - Revisited</strong></p>
         <p>One way in which the <span class="highlightme"><code>general linear model</code></span> differs from the 
            <span class="highlightme"><code>multiple regression model</code></span> is in terms of the number of dependent 
            variables that can be analyzed. 
            The \(Y\) vector of \(n\) observations of a single \(Y\) variable can be replaced by a \(Y\) matrix of \(n\) observations of \(m\) different \(Y\) variables. 
            Similarly, the \(w\) vector of regression coefficients for a single $Y$ variable can be replaced by a \(W\) matrix of regression coefficients, 
            with one vector of \(w\) coefficients for each of the m dependent variables. These substitutions yield what is sometimes called the multivariate regression model,
         </p>
         <p>$${Y}={XW}+{E}$$</p>
         <p>where \(Y\) is a matrix with series of multivariate measurements (each column being a set of measurements on one of the 
            dependent variables), \(X\) is a matrix of observations on independent variables that might be a 
            design matrix (each column being a set of observations on one of the independent variables), 
            \(W\) is a matrix containing parameters that are usually to be estimated and \(E\) is a matrix containing errors (noise). 
            The errors are usually assumed to be uncorrelated across measurements, and follow a multivariate normal distribution. 
            If the errors do not follow a multivariate normal distribution, generalized linear models may be used to relax assumptions about \(Y\) and \(W\).
         </p>
         <h3 id="generalizedlinearmodel">Generalized Linear Model</h3>
         <p>To summarize the basic idea, the <span class="highlightme"><code>generalized linear model</code></span> differs from 
            the <span class="highlightme"><code>general linear model</code></span> (of which multiple regression is a special case) in two major respects: 
         </p>
         <ol>
            <li>The distribution of the dependent or response variable _can be (explicitly) non-normal_, and does not have to be continuous, e.g., it can be binomial; </li>
            <li>The dependent variable values are predicted from a linear combination of predictor variables, 
               which are "connected" to the dependent variable via a <span class="highlightme"><code>link function</code></span>.
            </li>
         </ol>
         <ul>
            <li>General Linear Model</li>
         </ul>
         <p>$$y=\theta_0+\theta x_1+\theta x_2+...+\theta x_n$$</p>
         <ul>
            <li>Generalized Linear Model</li>
         </ul>
         <p>$$y=g(\theta_0+\theta x_1+\theta x_2+...+\theta x_n)$$
            $$gi(y)=\theta_0+\theta x_1+\theta x_2+...+\theta x_n$$
         </p>
         <p>where \(gi()\) is the inverse of \(g()\)</p>
         <p><strong>Link Function:</strong>
            Now inverse of \(g()\), say \(gi()\) is the link function.
         </p>
         <h3 id="generalizedadditivemodel">Generalized Additive Model:</h3>
         <p>We can combine the notion of <span class="highlightme"><code>additive models</code></span> with <span class="highlightme"><code>generalized linear models</code></span>, 
            to derive the notion of <span class="highlightme"><code>generalized additive models</code></span>, as:
         </p>
         <p>$$gi(y)=\theta_0+\theta f_1(x_1)+\theta f_2(x_2)+...+\theta f_n(x_n)$$</p>
         <p><a href="http://www.statsoft.com/Textbook/Generalized-Additive-Models">(link)</a></p>
         <a href="#top">Back to top of page</a>
         <hr />
         <h2 id="eigendecomposition">Eigen Decomposition</h2>
         <p>Matrices acts as linear transformations. Some matrices will rotate your space, 
            others will rescale it etc. So when we apply a matrix to a vector, 
            we end up with a transformed version of the vector. When we say that we ‘apply’ the matrix to the vector it means that we calculate the dot product of the matrix with the vector. 
         </p>
         <p><strong>Eigenvector:</strong> Now imagine that the transformation of the initial 
            vector gives us a new vector that has the exact same direction. 
            The scale can be different but the direction is the same. 
            Applying the matrix didn’t change the direction of the vector. 
            Thefere this type of initial vector is special and called an eigenvector of 
            the matrix.
         </p>
         <p>We can decompose the matrix \(A\) with eigenvectors and eigenvalues. 
            It is done with: $$A=V*\Sigma*V^{-1}$$, where \(\Sigma = diag(\lambda)\) and 
            each column of \(V\) is the eigenvector of \(A\).
         </p>
         <h3 id="realsymmetricmatrix">Real symmetric matrix</h3>
         <p>In the case of real symmetric matrices, the eigendecomposition 
            can be expressed as
            $$A=Q*\Sigma*Q^T$$
         </p>
         <p>where Q is the matrix with eigenvectors as columns and Λ is diag(λ).</p>
         <h3 id="whyeigenvalueandeigenvectorsaresoimportant">Why Eigenvalue and Eigenvectors are so important?</h3>
         <ul>
            <li>They are quite helpful for optimization algorithm or more clearly in
               constrained optimization problem. In optimization problem we are solving a 
               system of linear euaqtion. Typical <span class="highlightme"><span class="highlightme"><code>Gaussian Elemination 
               Technique</code></span></span> has 
               time complexity \(O(n^3)\) but this can be solved with Eigenvalue 
               Decomposition which needs \(O(n^2)\). So they are more efficient. [for more explanation follow the deeep learning lecture \(6\) of prof. Mitesh from IIT Madrass] 
            </li>
         </ul>
         <p>References:</p>
         <ul>
            <li>
               <p>Follow lecture 6 of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html">Prof.Mitesh_IITM</a></p>
            </li>
            <li><a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.7-Eigendecomposition/">Book</a></li>
            <li>
               <p>Book<a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.7-Eigendecomposition/">(Book_IMPORTANT link)</a></p>
            </li>
         </ul>
         <a href="#top">Back to top of page</a>
         <hr>
         <h2 id="pcaandsvd">PCA and SVD</h2>
         <p>Follow lecture 6 of <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html">Prof.Mitesh_IITM</a></p>
         <ul>
            <li>Eigenvectors can only be found for Square matrix. But, not ever square matrix has eigen vectors. </li>
            <li>All eigen vectors are perpendicular, i.e orthogonal.</li>
            <li>orthonormal vectors are orthogonal and they have unit length.</li>
            <li>If <code>V</code> is an orthonormal matrix then, <code>V'V=I</code></li>
         </ul>
         <p><strong>PCA</strong></p>
         <ul>
            <li>PCA decomposes a <strong>real, symmetric matrix $A$</strong> into <code>eigenvectors</code> and <code>eigenvalues</code>. </li>
            <li>Every <strong>real, symmetric matrix $A$</strong> can be decomposed into the following expression: <code>A=VSV'</code>. Where <code>V</code> is an orthogonal matrix. <code>S</code> is a diagonal matrix with all the eigen values.</li>
            <li>Though, any real symmetric matrix is <strong>guranteed</strong> to have an <strong>eigen decomposition</strong>, the decomposition may not be unique. </li>
            <li>If a matrix is not square, then it's eigen decomposition is not defined.</li>
            <li>A matrix is singular <strong>if and only if</strong>, any of the eigenvalue is zero.</li>
            <li>
               Consider A as real, symmetric matrix and $\lambda
               <em>
                  i$ are the eigen values.
                  <ul>
                     <li>if all $\lambda
               </em>
               i>0$, then A is called <code>positive definite</code> matrix.</li>
               <li>if all $\lambda_i>=0$, then A is called <code>positive semidefinite</code> (PSD) matrix.</li></ul>
            </li>
            <li>PSD matricies are interesting because the gurantee that <code>for all x</code>, $x'Ax>=0$. </li>
            <li>PSD additionally gurantees that if $x'Ax=0$ then $x=0$. <a href="https://medium.com/@SeoJaeDuk/principal-component-analysis-pooling-in-tensorflow-with-interactive-code-pcap-43aa2cee9bb">[source]</a>, <a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.12-Example-Principal-Components-Analysis/">[IMP_link]</a></li>
         </ul>
         <p><strong>SVD</strong> </p>
         <p><a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/">(IMP_source)</a></p>
         <blockquote>
            <blockquote>
               <p>A  is a matrix that can be seen as a linear transformation. This transformation can be decomposed in three sub-transformations: 1. rotation, 2. re-scaling, 3. rotation. These three steps correspond to the three matrices U, D, and V.</p>
            </blockquote>
         </blockquote>
         <p>$$A = U D V^T$$</p>
         <ul>
            <li>Every matrix can be seen as a linear transformation</li>
         </ul>
         <blockquote>
            <blockquote>
               <p>The transformation associated with diagonal matrices imply only a rescaling of each coordinate without rotation</p>
            </blockquote>
         </blockquote>
         <ul>
            <li>The SVD can be seen as the decomposition of one complex transformation in 3 simpler transformations (a rotation, a scaling and another rotation).</li>
            <li>SVD is more generic.</li>
            <li>SVD provides another way for factorizing a matrix, into <code>singular values</code> and <code>singular vectors</code>.</li>
            <li>Every real matrix has SVD but same is not true for PCA.</li>
            <li>If a matrix is not square then PCA not applicable.</li>
            <li>
               During PCA we write <code>A=VSV'</code>. However, for SVD we write <code>A=UDV'</code>, where A is <code>m x n</code>, U is <code>m x m</code>, D is <code>m x n</code> and V is <code>n x n</code>. 
               <ul>
                  <li>U, V orthogonal matricies.</li>
                  <li>D is diagonal matrix and not necessarily a square</li>
                  <li><code>diag(D)</code> are the <code>singular values</code> of the matrix A</li>
                  <li>Columns of <code>U</code> (<code>V</code>) are <code>left (right) singular vectors</code>.</li>
               </ul>
            </li>
         </ul>
         <p><strong>Relation between SVD and PCA</strong></p>
         <blockquote>
            <blockquote>
               <p>The matrices U, D and V can be found by transforming A in a square matrix and by computing the eigenvectors of this square matrix. The square matrix can be obtain by multiplying the matrix A by its transpose in one way or the other.</p>
               <ul>
                  <li>
                     The <code>left singular vectors</code> i.e. <code>U</code> are the eigen vectors of <code>AA'</code>. Similarly, the <code>right singular vectors</code> i.e. <code>V</code> are the eigen vectors of <code>A'A</code>. Note that A might not be a square matrix but <code>A'A</code> or <code>AA'</code> are both square.
                     <ul>
                        <li><code>A'A = VDV'</code> and <code>AA' = UDU'</code></li>
                        <li><code>D</code>  corresponds to the eigenvalues <code>AA'</code> or <code>A'A</code> which are the same.</li>
                     </ul>
                  </li>
               </ul>
            </blockquote>
         </blockquote>
         <p><a href="http://www.deeplearningbook.org/contents/linear_algebra.html">source: chapter 2, deep learning book - Goodfellow, p42</a>   </p>
         <p><strong>BONUS: Apply SVD on Images:</strong>
            <a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/">link</a>
         </p>
         <h2 id="determinant">Determinant</h2>
         <blockquote>
            <blockquote>
               <p>The determinant of a matrix A is a number corresponding to the multiplicative change you get when you transform your space with this matrix.</p>
            </blockquote>
         </blockquote>
         <ul>
            <li>A negative determinant means that there is a change in orientation (and not just a rescaling and/or a rotation).</li>
         </ul>
         <p><a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.11-The-determinant/">(Important_link)</a></p>
         <hr />
         <h2 id="constrainedoptimizationlagrangian">Constrained optimization (Lagrangian)</h2>
         <ul>
            <li>
               <strong>minimize</strong> $f(x)$ such that <strong><code>g(x)&lt;=0</code></strong> and $h(x)=0$. 
               <ul>
                  <li>Our target is to bring a new equation where we will combine $f(x), g(x), h(x)$ in a single equation. We will do this by introducing Lagrange Multiplier $\lambda$ and $\mu$. The new equation looks like: $L(x,\lambda,\mu)=f(x)+\lambda g(x)+ \mu h(x)$.</li>
               </ul>
            </li>
            <li>
               <strong>maximize</strong> $f(x)$ such that <strong><code>g(x)&gt;=0</code></strong> and $h(x)=0$. 
               <ul>
                  <li>Our target is to bring a new equation where we will combine $f(x), g(x), h(x)$ in a single equation. We will do this by introducing Lagrange Multiplier $\lambda$ and $\mu$. The new equation looks like: $L(x,\lambda,\mu)=f(x)+\lambda g(x)+ \mu h(x)$.</li>
               </ul>
            </li>
         </ul>
         <p><strong>NOTE:</strong> In the above formulation, pay special attention to the <code>minimize</code> and <code>maximize</code> kewords and the change in inequality constrains. So given any minimization or maximization problem, convert its constraints to $g(x)&lt;=0$ or $g(x)>=0$ accordingly and then formulate the Lagrangian form. The $h(x)=0$ may or may not be there. Finally apply KKT conditions for finding the solution. </p>
         <p><strong>KKT Conditions:</strong></p>
         <ul>
            <li>Stationarity $\nabla_x L(x,\lambda,\mu)=0$</li>
            <li>Primal feasibility, $g(x)&lt;=0$ (for minimization problem)</li>
            <li>Dual feasibility, $\lambda>=0, \mu>=0$</li>
            <li>Complementary slackness, $\lambda g(x) = 0$ and $\mu h(x)=0$</li>
         </ul>
         <p><a href="http://mat.gsia.cmu.edu/classes/QUANT/NOTES/chap4.pdf">notes</a>, <a href="http://cse.iitkgp.ac.in/~sourangshu/coursefiles/ML15A/svm.pdfz">sb slides</a>, <a href="https://www.cse.iitk.ac.in/users/rmittal/prev_course/s14/notes/lec11.pdf">iitK_notes</a>, <a href="http://www.math.ucsd.edu/~njw/Teaching/Math271C/Lecture_03.pdf">link2</a></p>
         <hr />
         <h2 id="solvelinearprogramming">Solve Linear Programming</h2>
         <blockquote>
            <blockquote>
               <p>min $c^Tx$ subject to: $Ax = b$, $x ≥ 0$.</p>
            </blockquote>
         </blockquote>
         <p>The linear programming problem is usually solved through the use of one of two
            algorithms: either simplex, or an algorithm in the family of interior point methods.
            In this article two representative members of the family of interior point methods are
            introduced and studied. We discuss the design of these interior point methods on a high
            level, and compare them to both the simplex algorithm and the original algorithms in
            nonlinear constrained optimization which led to their genesis.
            <a href="https://www.cs.toronto.edu/~robere/paper/interiorpoint.pdf">[survey paper]</a>
         </p>
         <p><strong>Simplex Method</strong></p>
         <blockquote>
            <p>Let $A, b, c$ be an instance of the LPP, defining a convex polytope in $R^n$. Then there exists an optimal solution to this program at one of the vertices of the <strong>polytope</strong>.</p>
         </blockquote>
         <p>The simplex algorithm works roughly as follows. We begin with a feasible point at one
            of the vertices of the polytope. Then we “walk” along the edges of the polytope from vertex
            to vertex, in such a way that the value of the objective function monotonically decreases at
            each step. When we reach a point in which the objective value can decrease no more, we are
            finished. <a href="https://www.youtube.com/watch?v=vVzjXpwW2xI">(youtube)</a>
         </p>
         <p><a href="https://www.analyticsvidhya.com/blog/2017/02/lintroductory-guide-on-linear-programming-explained-in-simple-english/">(source)</a>, <a href="https://www.cs.toronto.edu/~robere/paper/interiorpoint.pdf">(link2)</a></p>
         <p><strong>Interior Point Method</strong></p>
         <p>Our above question about the complexity of the LPP was answered by Khachiyan in 1979. He demonstrated a worst-case polynomial time algorithm for linear programming dubbed the ellipsoid method [Kha79] in which the algorithm moved across the <strong>interior of the feasible region, and not along the boundary like simplex</strong>. Unfortunately the worst case running time for the ellipsoid method was high: $O(n^6L^2)$, where n is the number of variables in the problem
            and L is the number of the bits in the input. Moreover, this method tended to approach the
            worst-case complexity on nearly all inputs, and so the simplex algorithm remained dominant
            in practice. This algorithm was only partially satisfying to researchers: was there a worst-case
            polynomial time algorithm for linear programming which had a performance that rivalled
            the performance of simplex on day-to-day problems?
            This question was answered by Karmarkar in 1984. He produced a polynomial-time
            algorithm — soon called the projective algorithm — for linear programming that ran in
            much better time: $O(n^{3.5}L^2)$ [Kar84]. 
         </p>
         <p><a href="https://www.inf.ethz.ch/personal/fukudak/lect/opt2011/aopt11note4.pdf">(link1)</a></p>
         <p><strong>First Order Method -  The KKT Conditions and Duality Theory</strong></p>
         <hr />
         <h2 id="gradientdecent">Gradient Decent</h2>
         <blockquote>
            <blockquote>
               <p>The main idea is that the sign of the derivative of the function at a specific value of x tells you if you need to increase (if negative slope) or decrease (if positive slope) x to reach the minimum. When the slope is near 0, the minimum should have been reached. 
                  <a href="https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.12-Example-Principal-Components-Analysis/">(Minimizing Function)</a>
               </p>
            </blockquote>
         </blockquote>
         <p>There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.</p>
         <p><strong>Batch gradient descent</strong></p>
         <p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ for the entire training dataset:</p>
         <p>$$\theta = \theta - \eta \cdot \nabla_\theta J( \theta)$$</p>
         <p>As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory. </p>
         <pre><code class="python language-python">for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad
</code></pre>
         <p><strong>Stochastic gradient descent:</strong></p>
         <p>Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example $x^{(i)}$ and label $y^{(i)}$:</p>
         <p>$$\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})$$</p>
         <p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. </p>
         <pre><code class="python language-python">for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad
</code></pre>
         <p><strong>Mini-batch gradient descent</strong>
            Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples:
         </p>
         <p>$$\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})$$</p>
         <pre><code class="python language-python">for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad
</code></pre>
         <p><strong>Momentum</strong></p>
         <p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.</p>
         <p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction $\gamma$ of the update vector of the past time step to the current update vector:</p>
         <p>$$v<em>t = \gamma v</em>{t-1} + \eta \nabla<em>\theta J( \theta)$$ <br />
            $$\theta = \theta - v</em>t$$
         </p>
         <p>Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. γ&lt;1). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>
         <p><a href="http://ruder.io/optimizing-gradient-descent/">(more details, see Sebastian Ruder blog)</a></p>
         <hr />
         <h2 id="distribution">Distribution</h2>
         <p>While the concept of probability gives us the mathematical calculations, distributions help us actually visualize what’s happening underneath.</p>
         <p><a href="https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/">(AVB)</a></p>
         <h3 id="uniformdistribution">Uniform Distribution</h3>
         <p>A variable X is said to be uniformly distributed if the density function is:</p>
         <p>$$f(x) = \frac{1}{b-a}$$ </p>
         <p>where $-\infty<a&lt;=x&lt;=b&lt;\infty$</p>
         <h3 id="bernoullidistribution">Bernoulli Distribution</h3>
         <p><strong>Story:</strong> All you cricket junkies out there! At the beginning of any cricket match, how do you decide who is going to bat or ball? A toss! It all depends on whether you win or lose the toss, right? Let’s say if the toss results in a head, you win. Else, you lose. There’s no midway.</p>
         <p><strong>Formulation:</strong> A Bernoulli distribution has only two possible outcomes, namely <code>1 (success)</code> and <code>0 (failure)</code>, and a <code>single trial</code>. So the random variable $X$ which has a Bernoulli distribution can take value 1 with the probability of <em>success</em>, say <code>p</code>, and the value 0 with the probability of <em>failure</em>, say <code>q</code> or <code>1-p</code>.</p>
         <p>The probability mass function is given by: </p>
         <p>$$p^x(1-p)^{1-x}$$ <br />
            where $x \in (0, 1)$.
         </p>
         <p>The expected value of a random variable X from a Bernoulli distribution is found as follows:</p>
         <p>$$E(X) = 1<em>p + 0</em>(1-p) = p$$</p>
         <p>The variance of a random variable from a bernoulli distribution is:</p>
         <p>$$V(X) = E(X^2)- [E(X)]^2 = p - p^2 = p(1-p)$$</p>
         <h3 id="binomialdistribution">Binomial Distribution</h3>
         <p><strong>Story:</strong> Let’s get back to cricket.  Suppose that you won the toss today and this indicates a successful event. You toss again but you lost this time. If you win a toss today, this does not necessitate that you will win the toss tomorrow. Let’s assign a random variable, say X, to the number of times you won the toss. What can be the possible value of X? It can be any number depending on the number of times you tossed a coin.</p>
         <p><strong>Formulation:</strong> An experiment with only two possible outcomes repeated n number of times is called binomial. The parameters of a binomial distribution are n and p where n is the total number of trials and p is the probability of success in each trial. If <code>x</code> is the total number fof success, we can write:</p>
         <p>$$P(x) = \binom{n}{x} p^x (1-p)^{n-x}$$</p>
         <p>On the basis of the above explanation, the properties of a Binomial Distribution are:</p>
         <ul>
            <li>Each trial is independent.</li>
            <li>There are only two possible outcomes in a trial- either a success or a failure.</li>
            <li>A total number of n identical trials are conducted.</li>
            <li>The probability of success and failure is same for all trials. (Trials are identical.)</li>
            <li>Mean:  $\mu = n*p$</li>
            <li>Variance: $Var(X) = n<em>p</em>(1-p)$</li>
         </ul>
         <h3 id="normaldistribution">Normal Distribution:</h3>
         <p>Normal distribution represents the behavior of most of the situations in the universe (That is why it’s called a “normal” distribution. I guess!). </p>
         <blockquote>
            <blockquote>
               <p>The large sum of (small) random variables often turns out to be normally distributed, contributing to its widespread application ~ Central Limit Theoram</p>
            </blockquote>
         </blockquote>
         <p>Any distribution is known as Normal distribution if it has the following characteristics:</p>
         <ul>
            <li><strong>The mean, median and mode of the distribution coincide.</strong></li>
            <li>The curve of the distribution is bell-shaped and symmetrical about the line $x=\mu$.</li>
            <li>The total area under the curve is <code>1</code>.</li>
            <li>Exactly half of the values are to the left of the center and the other half to the right.</li>
         </ul>
         <p>A normal distribution is highly different from Binomial Distribution. However, if the number of trials approaches infinity then the shapes will be quite similar.</p>
         <p>The PDF of a random variable X following a normal distribution is given by:</p>
         <p>$$P(x) = \frac{1}{\sqrt{2\pi\sigma}}\exp{{-\frac{(x-\mu)^2}{2\sigma^2}}}$$</p>
         <p>where $-\infty<x&lt;\infty$</p>
         <p>A <strong>standard normal distribution looks as follows:</strong></p>
         <p>$$P(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$$</p>
         <h3 id="poissondistribution">Poisson Distribution</h3>
         <p><strong>Story:</strong> Suppose you work at a call center, approximately how many calls do you get in a day? It can be any number. Now, the entire number of calls at a call center in a day is modeled by Poisson distribution. Some more examples are</p>
         <ul>
            <li>The number of emergency calls recorded at a hospital in a day.</li>
            <li>The number of thefts reported in an area on a day.</li>
            <li>The number of customers arriving at a salon in an hour.</li>
         </ul>
         <p>Poisson Distribution is applicable in situations where events occur at random points of time and space wherein our interest lies only in the number of occurrences of the event.</p>
         <p>A distribution is called Poisson distribution when the following assumptions are valid:</p>
         <ol>
            <li>Any successful event should not influence the outcome of another successful event.</li>
            <li>The probability of success over a short interval must equal the probability of success over a longer interval.</li>
            <li>The probability of success in an interval approaches zero as the interval becomes smaller.</li>
         </ol>
         <p><strong>Formulation:</strong> Now, if any distribution validates the above assumptions then it is a Poisson distribution. Some notations used in Poisson distribution are:</p>
         <ul>
            <li>$\lambda$ is the rate at which an event occurs,</li>
            <li>$t$ is the length of a time interval,</li>
            <li>$X$ is the number of events in that time interval.</li>
         </ul>
         <p>Here, $X$ is called a Poisson Random Variable and the probability distribution of $X$ is called Poisson distribution.</p>
         <p>Let $\mu$ denote the mean number of events in an interval of length t. Then, $\mu = \lambda*t$.</p>
         <p>The PMF of X following a Poisson distribution is given by:</p>
         <p>$$P(X=x)=e^{-\mu} \frac{\mu^x}{x!}$$</p>
         <p>where $x=0,1,2,3,...$</p>
         <ul>
            <li>Mean: $E(X) = \mu$</li>
            <li>Variance: $Var(X) = \mu$</li>
         </ul>
         <h3 id="exponentialdistribution">Exponential Distribution</h3>
         <p><strong>Story:</strong> Let’s consider the call center example one more time. <strong>What about the interval of time between the calls?</strong> Here, exponential distribution comes to our rescue. Exponential distribution models the interval of time between the calls.</p>
         <p>Other examples are:</p>
         <ol>
            <li>Length of time beteeen metro arrivals,</li>
            <li>Length of time between arrivals at a gas station</li>
            <li>The life of an Air Conditioner</li>
         </ol>
         <p>Exponential distribution is widely used for survival analysis. From the expected life of a machine to the expected life of a human, exponential distribution successfully delivers the result.</p>
         <p><strong>Formulation:</strong> A random variable $X$ is said to have an exponential distribution with PDF:</p>
         <p>$$f(x) = { \lambda e^{-\lambda x} $$</p>
         <p>where $x ≥ 0$ and parameter $\lambda>0$ which is also called the rate.</p>
         <p>For survival analysis, λ is called the failure rate of a device at any time t, given that it has survived up to t.</p>
         <p>Mean and Variance of a random variable X following an exponential distribution:</p>
         <ul>
            <li>Mean: $E(X) = \frac{1}{\lambda}$</li>
            <li>Variance: $Var(X) = \frac{1}{\lambda^2}$</li>
         </ul>
         <h3 id="gammadistribution">Gamma Distribution:</h3>
         <p>We now define the gamma distribution by providing its PDF: 
            A continuous random variable $X$ is said to have a gamma distribution with parameters $\alpha>0$ and $\lambda>0$, shown as $X \sim Gamma(\alpha,\lambda)$, if its PDF is given by:
         </p>
         <p>$$
            f_X(x) = \left{
            \begin{array}{l l}
            \frac{\lambda^{\alpha} x^{\alpha-1} e^{-\lambda x}}{\Gamma(\alpha)} \hspace {5pt} x > 0\
            0 \hspace{56pt} \textrm{otherwise}
            \end{array}\right.
            $$
         </p>
         <p>If we let $\alpha=1$, we obtain:</p>
         <p>$$
            f_X(x) = \left{
            \begin{array}{l l}
            \lambda e^{-\lambda x} \hspace{20pt} x > 0\
            0 \hspace{41pt} \textrm{otherwise}
            \end{array}\right.
            $$
         </p>
         <p>Thus, we conclude $Gamma(1,\lambda)=Exponential(\lambda)$. More generally, if you sum n independent $Exponential(\lambda)$ random variables, then you will get a $Gamma(n,\lambda)$ random variable.</p>
         <p><a href="https://www.probabilitycourse.com/chapter4/4_2_4_Gamma_distribution.php">(gamma)</a></p>
         <h4 id="howallthedistributionsarerelated">How all the distributions are related?</h4>
         <p><img src="images/distribution.png" alt="distribution" />
            <a href="https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/">(blog)</a>
         </p>
         <p><strong>Relation between Exponential and Poisson Distribution:</strong></p>
         <blockquote>
            <p>If the times between random events follow exponential distribution with rate $\lambda$, then the total number of events in a time period of length t follows the Poisson distribution with parameter $\lambda t$.</p>
         </blockquote>
         <hr />
         <h2 id="momentsshapeofyou">Moments - Shape of You !!</h2>
         <p>Moments try to measure the <strong>shape of the probability distribution function</strong>. </p>
         <ul>
            <li>The zeroth moment is the total probability of the distribution which is <strong>1</strong>. </li>
            <li>The first moment is the <strong>mean</strong>. </li>
            <li>The second moment is the <strong>variance</strong>. </li>
            <li>The third moment is the <strong>skew</strong> which measures how lopsided the distribution is. </li>
            <li>The fourth moment is <strong>kurtosis</strong> which is the measure of <strong>how sharp is the peak of the graph</strong>.</li>
         </ul>
         <p><a href="https://medium.com/technology-nineleaps/basics-of-statistics-for-machine-learning-engineers-ii-d25c5a5dac67">(medium)</a></p>
         <p>Moments are important because, under some assumptions, moments are a good estimate of how the population probability distribution is based on the sample distribution. We can even have a good feel of how far off the population moments are from our sample moments under some realistic assumptions. And once the population moments are known that means the shape of the population probability distribution is known as well.</p>
         <hr />
         <h2 id="centrallimittheoram">Central Limit Theoram</h2>
         <blockquote>
            <blockquote>
               <p>In probability theory, the central limit theorem (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a "bell curve") even if the original variables themselves are not normally distributed. </p>
            </blockquote>
         </blockquote>
         <p>The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">(wiki)</a></p>
         <p><strong>Formulation:</strong>
            Suppose that $[X<em>1, X</em>2 ,...,X<em>n]$ are i.i.d. random variables with expected values $E(X</em>{\large i})=\mu &lt; \infty$ and variance $\mathrm{Var}(X<em>{\large i})=\sigma^2 &lt; \infty$. Then as we saw above, the sample mean $\overline{X}={\large\frac{X</em>1+X<em>2+...+X</em>n}{n}}$ has mean $E(\overline{X})=\mu$ and variance $\mathrm{Var}(\overline{X})={\large \frac{\sigma^2}{n}}$. Thus the normalized random variable is: 
         </p>
         <p>$$Z<em>{\large n}=\frac{\overline{X}-\mu}{ \sigma / \sqrt{n}}=\frac{X</em>1+X<em>2+...+X</em>{\large n}-n\mu}{\sqrt{n} \sigma}$$</p>
         <p>which has mean $E(Z<em>n)=0$ and variance $Var(Z</em>n)=1$, i.e the random variable $Z_n$ converges in distribution to the standard normal random variable as n goes to infinity, i.e</p>
         <p>$$\lim<em>{n \rightarrow \infty} P(Z</em>{\large n} \leq x)=\Phi(x), \qquad \textrm{ for all }x \in \mathbb{R}$$,</p>
         <p>where $\Phi(x)$ is the standard normal CDF.</p>
         <p>Let's say $X$ follows Binomial distribution. Then as ${n \rightarrow \infty}$, the distribution of $Z_n$ looks like this:</p>
         <p><img src="images/CLT.png" alt="CLT" /></p>
         <p><a href="https://www.probabilitycourse.com/chapter7/7_1_2_central_limit_theorem.php">(source)</a></p>
         <hr />
         <h2 id="lawoflargenumber">Law of Large Number</h2>
         <hr />
         <h2 id="roccurveanalysis">ROC Curve Analysis</h2>
         <h3 id="roccurve">ROC curve</h3>
         <p>An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:</p>
         <ul>
            <li>
               <p>True Positive Rate:</p>
               <ul>
                  <li>True Positive Rate (TPR) is a synonym for recall and is therefore defined as follows:</li>
               </ul>
               <p>$$TPR = \frac{TP} {TP + FN}$$</p>
            </li>
            <li>
               <p>False Positive Rate</p>
            </li>
         </ul>
         <p>$$FPR = \frac{FP} {FP + TN}$$</p>
         <p>To compute the points in an ROC curve, we could evaluate a logistic regression model many times with different classification thresholds, but this would be inefficient. Fortunately, there's an efficient, sorting-based algorithm that can provide this information for us, called AUC.</p>
         <h3 id="aucareaundertheroccurve">AUC: Area Under the ROC Curve</h3>
         <p>AUC stands for "Area under the ROC Curve." That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).</p>
         <p>AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. 
            <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">(source)</a>
         </p>
         <hr />
         <h2 id="whatispvalues">What is p-values?</h2>
         <p><strong>How it is decided for rejecting null hypothesis? Why it's called null hypothesis?</strong></p>
         <ul>
            <li><code>Null hypothesis</code> means the hypothesis which you want to nullify. So there is an <code>alternate hypothesis</code>, which will be accepted if the null hypothesis is rejected.</li>
            <li>A <code>p-value</code> is the probability of finding some sample outcome or a more extreme one if the null hypothesis is true.</li>
            <li>
               <strong>Example:</strong> I want to know if happiness is related to wealth among Dutch people. One approach to find this out is to formulate a null hypothesis. Since “related to” is not precise, we choose the opposite statement as our null hypothesis:
               >   The correlation between wealth and happiness is zero among all Dutch people.
               <ul>
                  <li>We'll now try to refute this hypothesis in order to demonstrate that happiness and wealth are related all right.</li>
                  <li>Now, we can't reasonably ask all 17,142,066 Dutch people how happy they generally feel. So we'll ask a sample (say, 100 people) about their wealth and their happiness. The correlation between happiness and wealth turns out to be 0.25 in our sample. Now we've one problem: sample outcomes tend to differ somewhat from population outcomes. </li>
                  <li><strong>How we can ever say anything about our population if we only have a tiny sample from it.</strong> </li>
                  <li>So how does that work? Well, basically, some sample outcomes are highly unlikely given our null hypothesis.</li>
                  <li>If our population correlation really is zero, then we can find a sample correlation of 0.25 in a sample of N = 100. The probability of this happening is only 0.012. So it's very unlikely. A reasonable conclusion is that our population correlation wasn't zero after all. </li>
                  <li><strong>Conclusion:</strong> we reject the null hypothesis. Given our sample outcome, we no longer believe that happiness and wealth are unrelated. However, we still can't state this with certainty.</li>
                  <li><a href="https://www.spss-tutorials.com/null-hypothesis/">source</a>, <a href="https://onlinecourses.science.psu.edu/statprogram/node/138/">link2</a></li>
               </ul>
            </li>
         </ul>
         <h2 id="recommendationsystem">Recommendation System:</h2>
         <h3 id="contentbasedrecommendation">Content based recommendation:</h3>
         <h4 id="usingfeaturevectorandregressionbasedanalysis">Using Feature vector and regression based analysis</h4>
         <p>Here the assumption is that for each of the <code>item</code> you have the corresponding features available. </p>
         <p><strong>Story:</strong> Below are a table, of movie-user rating. 5 users and 4 movies. And the ratings are also provided (1-5) for some of them. We want to predict the rating for all the <code>?</code> unknown. It means that user $u<em>j$ has not seen that movie $m</em>i$ and if we can predict the rating for cell (i,j) then it will help us to decide whether or not to recommend the movie to the user $u_j$. For example, cell (3,4) is unknown. If by some algorithm, we can predict the rating for that cell, say the predicted rating is 4, it means if the user <strong>would have seen this movie, then he would have rated the movie 4</strong>. So we must definitely recommend this movie to the user. </p>
         <p>|    | u1 | u2 | u3 | u4 | u5 |
            |:--:|:--:|:--:|:--:|----|:--:|
            | m1 |  4 |  5 |  ? | 1  |  2 |
            | m2 |  ? |  4 |  5 | 0  |  ? |
            | m3 |  0 |  ? |  1 | ?  |  5 |
            | m4 | 1  | 0  | 2  | 5  | ?  |
         </p>
         <p>Now for content based movie recommendation it's assumed that the features available for the content. For example if we see closely, then we can see the following patterns in the table. The bold ratings have segmented the table into 4 sub parts based on rating clustering.</p>
         <p>|    | u1 | u2 | u3 | u4 | u5 |
            |:--:|:--:|:--:|:--:|----|:--:|
            | m1 |  <strong>4</strong> |  <strong>5</strong> |  <strong>?</strong> | 1  |  2 |
            | m2 |  <strong>?</strong> |  <strong>4</strong> |  <strong>5</strong> | 0  |  ? |
            | m3 |  0 |  ? |  1 | <strong>?</strong>  |  <strong>5</strong> |
            | m4 | 1  | 0  | 2  | <strong>5</strong>  | <strong>?</strong>  |
         </p>
         <p>as if movie $m<em>1$, $m</em>2$ belong to type 1 (say romance) and $m<em>3$, and $m</em>4$ belong to type 2 (say action) and there is a clear discrimination is the rating as well.</p>
         <p>Now for content based recommendation this types are available and then the datasets actually looks as follows</p>
         <p>|    | u1 | u2 | u3 | u4 | u5 | T1  | T2  |
            |:--:|:--:|:--:|:--:|----|:--:|-----|-----|
            | m1 |  4 |  5 |  ? | 1  |  2 | 0.9 | 0.1 |
            | m2 |  ? |  4 |  5 | 0  |  ? | 0.8 | 0.2 |
            | m3 |  0 |  ? |  1 | ?  |  5 | 0.2 | 0.8 |
            | m4 | 1  | 0  | 2  | 5  | ?  | 0.1 | 0.9 |
         </p>
         <p>where $T<em>1$ and $T</em>2$ columns are already known. Then for each of the user we can learn a regression problem with the known rating as the target vector $u<em>j$ and $A = [T</em>1,T<em>2]$
            is the feature matrix and we need to learn the $\theta</em>j$ for user $j$ such that $A \theta<em>j = u</em>j$. Create the loss function and solve the optimization problem.
         </p>
         <p><a href="https://www.youtube.com/watch?v=c0ZPDKbYzx0&amp;list=PLnnr1O8OWc6ZYcnoNWQignIiP5RRtu3aS&amp;index=2">(Content Based Recom -A.Ng)</a>,
            <a href="https://www.youtube.com/watch?v=2uxXPzm-7FY&amp;index=42&amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV">(MMD - Stanford )</a>
         </p>
         <h3 id="colaborativefiltering">Colaborative FIltering</h3>
         <p><strong>Story:</strong> Unlike the <code>content based recommendation</code>, where the feature columns ($T<em>1$, $T</em>2$) were already given, here the features are not present. Rather they are being learnt by the algorithm. Here we assume $\theta<em>j$ is given i.e we know the users liking for $T</em>1$ and $T<em>2$ movies and almost similarly we formulate the regression problem but now we try to estimate the feature columns $T</em>k$. Then using the learnt feature we estimate the unknown ratings and then recommend those movies. Here knowing $\theta_j$
            means that each user has given inofrmation regarding his/her preferences based on subset of movies and thus all the users are colaboratively helping to learn the features.
         </p>
         <p>Naive Algorithm: </p>
         <ol>
            <li>Given $\theta<em>j$, learn features $T</em>k$. [loss function $J(T_k)$]</li>
            <li>Given $T<em>k$, learn $\theta</em>j$. [loss function $J(\theta_j)$] </li>
         </ol>
         <p>and start with a random initialization of $\theta$ and then move back and forth between step 1 and 2.</p>
         <p>Better Algorithm:</p>
         <ul>
            <li>combine step 1 and 2 into a single loss function $J(\theta<em>j,T</em>k)$ and solve that.</li>
         </ul>
         <h4 id="userusercf">User User CF</h4>
         <ul>
            <li>
               fix a similarity function (jaccard similarity, cosine similarity) for two vectors. and then pick any two users profile (their rating for different movies) and calculate the similarity function which helps you to cluster the users.
               <ul>
                  <li>jaccard similarity doesn;t consider the rating</li>
                  <li>cosine similarity consider the unknow entries as 0, which causes problem as rating range is (0-5).</li>
                  <li>cetered cosine similarity (pearson correlation): substract $\mu<em>{user}^{rating}$ from $rating</em>{user}$. </li>
                  <li>missing ratings are treated as average </li>
               </ul>
            </li>
         </ul>
         <h4 id="itemitemcf">Item Item CF</h4>
         <p>Almost similar to User User CF.</p>
         <p>Pros:</p>
         <ul>
            <li>works for any kind of item</li>
            <li>No need data for other user</li>
            <li>It's personalized recommendation as for each user a regression problem is learnt.</li>
            <li>No firt-rater problem, i.e. we can recommend an item to the user, as soon as it's available in the market.</li>
         </ul>
         <p>Cons:</p>
         <ul>
            <li>Finding the correcct feature is hard </li>
            <li>cold start problem for new user </li>
            <li>Sparsiry</li>
            <li>Popularity bias</li>
         </ul>
         <p><a href="https://www.youtube.com/watch?v=-Fptv3NZtmE&amp;index=4&amp;list=PLnnr1O8OWc6ZYcnoNWQignIiP5RRtu3aS">(Collaborative Filtering-A.Ng)</a>, <a href="https://www.youtube.com/watch?v=2uxXPzm-7FY&amp;index=42&amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV">(MMD - Stanford )</a></p>
         <h3 id="colaborativefilteringlowrankmatrixfactorizationsvd">Colaborative Filtering - Low Rank Matrix Factorization (SVD)</h3>
         <h3 id="evaluationofrecommendingsystem">Evaluation of Recommending System</h3>
         <p>RMSE: Root Mean Square Error. However it doesn't distinguish between high rating and low rating. </p>
         <ul>
            <li>Alternative: Precision $@k$ i.e. get the precision for top k items.</li>
         </ul>
         <h2 id="clusteringalgorithm">Clustering Algorithm</h2>
         <ul>
            <li>When data set is very large, can't fit into memory, then K means algorithm is not a good algorithm. Rather use Bradley-Fayyad-Reina (BFR) algorithm.</li>
         </ul>
         <h3 id="bfr">BFR</h3>
         <p>Assumption:</p>
         <ul>
            <li>Each cluster is normally distributed around a centroid in Eucledian space</li>
            <li>Normal distribution assumption implies that clusters looks like axis-aligned ellipses, i.e. standard deviation is maximum along one axis and minimum w.r.t other.</li>
         </ul>
         <p>[watch MMD Youtube]</p>
         <h2 id="wordembeddingimplementation">Word embedding - Implementation</h2>
         <p>Natural language processing systems traditionally treat words as discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that the model can leverage very little of what it has learned about 'cats' when it is processing data about 'dogs'.</p>
         <p>Word embeddings transform sparse vector representations of words into a dense, continuous vector space, enabling you to identify similarities between words and phrases — on a large scale — based on their context.</p>
         <p><code>Vector space models</code> (VSMs) represent (<code>embed</code>) words in a continuous vector space where semantically similar words are mapped to nearby points (<code>'are embedded nearby each other'</code>). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the <code>Distributional Hypothesis</code>, which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories: </p>
         <ul>
            <li><code>count-based methods</code> (e.g. Latent Semantic Analysis)</li>
            <li><code>predictive methods</code> (e.g. neural probabilistic language models).</li>
         </ul>
         <p>This distinction is elaborated in much more detail by Baroni et al., but in a nutshell: </p>
         <ul>
            <li>Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word. </li>
            <li>Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model).</li>
         </ul>
         <p>Neural probabilistic language models are traditionally trained using the maximum likelihood (ML) principle to maximize the probability of the next word $w_t$ (for "target") given the previous words $h$ (for "history") in terms of a softmax function</p>
         <p>$$
            P(w<em>t | h) = \text{softmax} (\text{score} (w</em>t, h))
            $$
            $$
            = \frac{\exp { \text{score} (w<em>t, h) } }
            {\sum</em>\text{Word w' in Vocab} \exp { \text{score} (w', h) } }
            $$
         </p>
         <p>where $score(w<em>t,h)$ computes the compatibility of word $w</em>t$  with the context $h$ (a dot product is commonly used). We train this model by maximizing its log-likelihood on the training set, i.e. by maximizing</p>
         <p>$$J<em>\text{ML} = \log P(w</em>t | h)$$
            $$= \text{score} (w<em>t, h) -
            \log \left( \sum</em>\text{Word w' in Vocab} \exp { \text{score} (w', h) } \right).
            $$
         </p>
         <p>This yields a <code>properly normalized probabilistic</code> model for language modeling. However this is very expensive, because we need to compute and normalize each probability using the score for all other $V$ words $w'$ in the current context $h$ , at every training step.</p>
         <p><img src="https://www.tensorflow.org/images/softmax-nplm.png" alt="image" height="400" width="200"/></p>
         <p><a href="https://www.tensorflow.org/tutorials/representation/word2vec">source_1</a></p>
         <h2 id="word2vecimplementation">Word2vec = Implementation</h2>
         <p>Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors:</p>
         <ul>
            <li>Continuous Bag-of-Words model (CBOW)</li>
            <li>Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.). </li>
         </ul>
         <p>Algorithmically, these models are similar, except that CBOW predicts target words (e.g. 'mat') from a sliding window of a source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. This inversion might seem like an arbitrary choice, but statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets. We will focus on the skip-gram model in the rest of this tutorial.</p>
         <p>For feature learning in word2vec we do not need a full probabilistic model. The CBOW and skip-gram models are instead trained using a binary classification objective (logistic regression) to discriminate the real target words $w_t$ from $k$ imaginary (noise) words $\tilde w$, in the same context. We illustrate this below for a CBOW model. For skip-gram the direction is simply inverted.</p>
         <p>Google's word2vec is one of the most widely used implementations due to its training speed and performance. Word2vec is a predictive model, which means that instead of utilizing word counts à la  latent Dirichlet allocation (LDA), it is trained to predict a target word from the context of its neighboring words. The model first encodes each word using one-hot-encoding, then feeds it into a hidden layer using a matrix of weights; the output of this process is the target word. The word embedding vectors are are actually the weights of this fitted model. To illustrate, here's a simple visual: </p>
         <p><img src="https://www.datascience.com/hs-fs/hubfs/Resources/Articles/nn_embed.png?t=1532623979827&amp;width=1414&amp;height=694&amp;name=nn_embed.png" alt="image" height="400" width="800"/></p>
         <p><img src="https://www.tensorflow.org/images/nce-nplm.png" alt="image" height="400" width="200"/></p>
         <p><a href="https://www.tensorflow.org/tutorials/representation/word2vec">source_1</a></p>
         <p><a href="https://www.datascience.com/resources/notebooks/word-embeddings-in-python">source_2</a></p>
         <p><a href="https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795">source_3</a></p>
         <h2 id="ldatopicmodelling">LDA, Topic Modelling</h2>
         <p>Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts.</p>
         <p>Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.</p>
         <p>Topic Models are very useful for the purpose for document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example – New York Times are using topic models to boost their user – article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.</p>
         <p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling1.png" alt="image" /></p>
         <p>You can try doing topic modelling using the following methods.</p>
         <ul>
            <li>Term Frequency and Inverse Document Frequency. </li>
            <li>Do Non negative Matrix Factorization (NMF)</li>
            <li>LDA. </li>
         </ul>
         <p>NMF is supposed to be a lot faster than LDA, but LDAs supposed to be more accurate. Problem is LDA takes a long time, unless you’re using distributed computing.</p>
         <p>LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.</p>
         <p>LDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. LDA converts this Document-Term Matrix into two lower dimensional matrices – M1 and M2.
            M1 is a document-topics matrix and M2 is a topic – terms matrix with dimensions ($N$,  $K$) and ($K$, $M$) respectively, where $N$ is the number of documents, $K$ is the number of topics and $M$ is the vocabulary size.
         </p>
         <p>It Iterates through each word <code>w</code> for each document <code>d</code> and tries to adjust the current topic – word assignment with a new assignment. A new topic <code>k</code> is assigned to word <code>w</code> with a probability P which is a product of two probabilities p1 and p2.</p>
         <p>For every topic, two probabilities p1 and p2 are calculated. </p>
         <ul>
            <li>P1: $p(topic<em>t | doc</em>d)$ = the proportion of words in document d that are currently assigned to topic t.</li>
            <li>P2: $p(word<em>w | topic</em>t)$ = the proportion of assignments to topic t over all documents that come from this word w.</li>
         </ul>
         <p>The current topic – word assignment is updated with a new topic with the probability, product of p1 and p2 . In this step, the model assumes that all the existing word – topic assignments except the current word are correct. This is essentially the probability that topic t generated word w, so it makes sense to adjust the current word’s topic with new probability.</p>
         <p>After a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. This is the convergence point of LDA.</p>
         <p><a href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/">AVB</a></p>
         <p><a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">EdwinChen</a></p>
         <p><a href="https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e">medium-PySparkImplementation</a></p>
         <h2 id="lstmdemystified">LSTM demystified</h2>
         <p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="image" 
            height="400" width="800"/></p>
         <p><strong>Core Idea:</strong>
            The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.
         </p>
         <p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" alt="image" height="400" width="800" /></p>
         <p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. It has <code>forget gate</code>, <code>input gate</code> and <code>output gate</code>.</p>
         <p><strong>Forget Gate:</strong></p>
         <p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="image" height="400" width="800"/></p>
         <p>It controls how much old information (or old memory or state) you want to retain or not. $f_t\in(0,1)$ and thus it maintains a proportion of old memory.</p>
         <p><strong>Input Gate:</strong></p>
         <p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="image" height="400" width="800"/></p>
         <p>The next step is to decide what new information we’re going to store in the cell state. This has two parts. </p>
         <ul>
            <li>First, a sigmoid layer called the “input gate layer” decides which values we’ll update. </li>
            <li>Second, a tanh layer creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.</li>
         </ul>
         <p><strong>Cell Update:</strong></p>
         <p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="image" height="400" width="800"/></p>
         <p>We multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add $i<em>t*\tilde{C}</em>t$. This is the new candidate values, scaled by how much we decided to update each state value.</p>
         <p><strong>Output Gate:</strong></p>
         <p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="image" height="400" width="800"/></p>
         <p>Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. </p>
         <ul>
            <li>First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. </li>
            <li>Second, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.</li>
         </ul>
         <p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">chris_olah</a></p>
         <p><a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714">medium</a></p>
         <p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">a_karpathy</a></p>
         <h3 id="whytheinputoutputgatesareneeded">why the input/output gates are needed ?</h3>
         <p><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">source_1</a></p>
         <h2 id="adaboostobjectivefunction">Adaboost objective function</h2>
         <h2 id="vanishinggradientproblemingeneral">Vanishing Gradient Problem in general</h2>
         <p><a href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b">source</a></p>
         <h2 id="whatisvanishinggradientprobleminrnn">What is vanishing gradient problem in RNN?</h2>
         <p>If you don’t already know, the vanishing gradient problem arises when,during backprop, the error signal used to train the network exponentially decreases the further you travel backwards in your network. The effect of this is that the layers closer to your input don’t get trained.</p>
         <p>To understand why LSTMs help, we need to understand the problem with vanilla RNNs. In a vanilla RNN, the hidden vector and the output is computed as such:</p>
         <p>$$h<em>t = tanh(W</em>Ix<em>t + W</em>Rh<em>{t-1})$$
            $$y</em>t = W<em>Oh</em>t$$
         </p>
         <p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" alt="image" height="400" width="800"/></p>
         <p>To do backpropagation through time to train the RNN, we need to compute the gradient of $E$ with respect to $W_R$. The overall error gradient is equal to the sum of the error gradients at each time step. For step $t$, we can use the multivariate chain rule to derive the error gradient as:</p>
         <p>$$
            \frac{\partial E<em>t}{\partial W</em>R} = \sum^{t}<em>{i=0} \frac{\partial E</em>t}{\partial y<em>t}\frac{\partial y</em>t}{\partial h<em>t}\frac{\partial h</em>t}{\partial h<em>i}\frac{\partial h</em>i}{\partial W_R}
            $$
         </p>
         <p>Now everything here can be computed pretty easily except the term 
            $\frac{\partial h<em>t}{\partial h</em>i}$, which needs another chain rule application to compute:
         </p>
         <p>$$
            \frac{\partial h<em>t}{\partial h</em>i} = \frac{\partial h<em>t}{\partial h</em>{t-1}}\frac{\partial h<em>{t-1}}{\partial h</em>{t-2}}...\frac{\partial h<em>{i+1}}{\partial h</em>i} = \prod^{t-1}<em>{k=i} \frac{\partial h</em>{k+1}}{\partial h_k}
            $$
         </p>
         <p>Now let us look at a single one of these terms by taking the derivative of $h<em>{k+1}$ with respect to $h</em>k$ (where $diag$ turns a vector into a diagonal matrix):</p>
         <p>$$
            \frac{\partial h<em>{k+1}}{\partial h</em>k} =  diag(f'(W<em>Ix</em>i + W<em>Rh</em>{i-1}))W_R
            $$
         </p>
         <p>Thus, if we want to backpropagate through $k$ timesteps, this gradient will be :</p>
         <p>$$
            \frac{\partial h<em>{k}}{\partial h</em>1} =  \prod<em>i^k diag(f'(W</em>Ix<em>i + W</em>Rh<em>{i-1}))W</em>R
            $$
         </p>
         <p>As shown in this paper, if the dominant eigenvalue of the matrix $W<em>R$
            is greater than 1, the gradient explodes. If it is less than 1, the gradient vanishes.2 The fact that this equation leads to either vanishing or exploding gradients should make intuitive sense. Note that the values of $f'(x)$ will always be less than 1. So if the magnitude of the values of $W</em>R$ are too small, then inevitably the derivative will go to 0. The repeated multiplications of values less than one would overpower the repeated multiplications of $W<em>R$
            . On the contrary, make $W</em>R$ too big and the derivative will go to infinity since the exponentiation of $W_R$ will overpower the repeated multiplication of the values less than 1. In practice, the vanishing gradient is more common, so we will mostly focus on that.
         </p>
         <p>The derivative $\frac{\partial h<em>{k}}{\partial h</em>1}$ is essentially telling us how much our hidden state at time $k$ will change when we change the hidden state at time 1 by a little bit. According to the above math, if the gradient vanishes it means the earlier hidden states have no real effect on the later hidden states, meaning no long term dependencies are learned! This can be formally proved, and has been in many papers, including the original LSTM paper.</p>
         <p><a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html">source_1</a></p>
         <h2 id="howdoeslstmsolvevanishinggradientproblem">How does LSTM solve vanishing gradient problem?</h2>
         <p>the vanishing gradient problem arises when, during backprop, the error signal used to train the network exponentially decreases the further you travel backwards in your network. The effect of this is that the layers closer to your input don’t get trained. In the case of RNNs (which can be unrolled and thought of as feed forward networks with shared weights) this means that you don’t keep track of any long term dependencies. This is kind of a bummer, since the whole point of an RNN is to keep track of long term dependencies. The situation is analogous to having a video chat application that can’t handle video chats!</p>
         <p><strong>LSTM Equation Reference:</strong> Quickly, here is a little review of the LSTM equations, with the biases left off </p>
         <ul>
            <li>$f<em>t=\sigma(W</em>f[h<em>{t-1},x</em>t])$</li>
            <li>$i<em>t=\sigma(W</em>i[h<em>{t-1},x</em>t])$</li>
            <li>$o<em>t=\sigma(W</em>o[h<em>{t-1},x</em>t])$</li>
            <li>$\widetilde{C}<em>t=tanh(W</em>C[h<em>{t-1},x</em>t])$</li>
            <li>$C<em>t=f</em>tC<em>{t-1} + i</em>t\widetilde{C}_t$</li>
            <li>$h<em>t=o</em>ttanh(C_t)$</li>
         </ul>
         <p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="image" height="400" width="800"/></p>
         <p>As we can see above (previous question), the biggest culprit in causing our gradients to vanish is that dastardly recursive derivative we need to compute: 
            $\frac{\partial h<em>t}{\partial h</em>i}$. If only this derivative was ‘well behaved’ (that is, it doesn’t go to 0 or infinity as we backpropagate through layers) then we could learn long term dependencies!
         </p>
         <p><strong>The original LSTM solution:</strong> The original motivation behind the LSTM was to make this recursive derivative have a constant value. If this is the case then our gradients would neither explode or vanish. How is this accomplished? As you may know, the LSTM introduces a separate cell state $C<em>t$. In the original 1997 LSTM, the value for $C</em>t$ depends on the previous value of the cell state and an update term weighted by the input gate value. </p>
         <p>$$C<em>t = C</em>{t-1} + i\widetilde{C}_t$$</p>
         <p>This formulation doesn’t work well because the cell state tends to grow uncontrollably. In order to prevent this unbounded growth, a forget gate was added to scale the previous cell state, leading to the more modern formulation:</p>
         <p>$$C<em>t = fC</em>{t-1} + i\widetilde{C}_t$$</p>
         <p><strong>A common misconception:</strong> Most explanations for why LSTMs solve the vanishing gradient state that under this update rule, the recursive derivative is equal to 1 (in the case of the original LSTM) or f
            (in the case of the modern LSTM)3 and is thus well behaved! One thing that is often forgotten is that f, i, and $\widetilde{C}<em>t$
            are all functions of $C</em>t$, and thus we must take them into consideration when calculating the gradient.
         </p>
         <p>The reason for this misconception is pretty reasonable. In the original LSTM formulation in 1997, the recursive gradient actually was equal to 1. The reason for this is because, in order to enforce this constant error flow, the gradient calculation was truncated so as not to flow back to the input or candidate gates. So with respect to $C_{t-1}$ they could be treated as constants. In fact truncating the gradients in this way was done up till about 2005, until the publication of this paper by Alex Graves. Since most popular neural network frameworks now do auto differentiation, its likely that you are using the full LSTM gradient formulation too! So, does the above argument about why LSTMs solve the vanishing gradient change when using the full gradient? The answer is no, actually it remains mostly the same. It just gets a bit messier. </p>
         <p><strong>Looking at the full LSTM gradient:</strong> To understand why nothing really changes when using the full gradient, we need to look at what happens to the recursive gradient when we take the full gradient. For the derivation check this <a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html">link</a>. </p>
         <p><em>Note:</em> One important thing to note is that the values 
            $f<em>t$, $o</em>t$, $i<em>t$, and $\widetilde{C}</em>t$ are things that the network learns to set (conditioned on the current input and hidden state). Thus, in this way the network learns to decide when to let the gradient vanish, and when to preserve it, by setting the gate values accordingly!
         </p>
         <p>This might all seem magical, but it really is just the result of two main things:</p>
         <ul>
            <li>The additive update function for the cell state gives a derivative thats much more ‘well behaved’</li>
            <li>The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.</li>
         </ul>
         <h3 id="thereisstillachanceofgradientvanishingbutthemodelwouldregulateitsforgetgatevaluetopreventthatfromhappening">There is still a chance of gradient vanishing, but the model would regulate its forget gate value to prevent that from happening?</h3>
         <p>Yes this is correct. If the model is dumb and always sets its forget gate to a low value, then the gradient will vanish. Since the forget gate value is a learnable function, we hope that it learns to regulate the forget gate in a way that improves task performance</p>
         <h3 id="howdoesthispreventgradientexplosionfrommyunderstandinggradientclippingisappliedtopresentthatfromhappening">How does this prevent gradient explosion? From my understanding gradient clipping is applied to present that from happening.</h3>
         <p>This does not really help with gradient explosions, if your gradients are too high there is little that the LSTM gating functions can do for you. There are two standard methods for preventing exploding gradients: Hope and pray that you don't get them while training, or use gradient clipping.
            The latter has a greater success rate, however I have had some success with the former, YMMV.
         </p>
         <h2 id="howdoescnnsolvevanishinggradientproblem">How does CNN solve vanishing gradient problem?</h2>
         <p>RNNs unfolded are deep in common application scenarios, thus prone to severer vanishing gradient problems. CNNs in typical application scenarios are shallower, but still suffer from the same problem with gradients.</p>
         <p>The most recommended approaches to overcome the vanishing gradient problem are:</p>
         <ul>
            <li>Layerwise pre-training</li>
            <li>Choice of the activation function</li>
         </ul>
         <p>You may see that the state-of-the-art deep neural network for computer vision problem (like the ImageNet winners) have used convolutional layers as the first few layers of the their network, but it is not the key for solving the vanishing gradient. The key is usually training the network greedily layer by layer. Using convolutional layers have several other important benefits of course. Especially in vision problems when the input size is large (the pixels of an image), using convolutional layers for the first layers are recommended because they have fewer parameters than fully-connected layers and you don't end up with billions of parameters for the first layer (which will make your network prone to overfitting).</p>
         <p>However, it has been shown (like this paper) for several tasks that using Rectified linear units alleviates the problem of vanishing gradients (as oppose to conventional sigmoid functions).</p>
         <p><a href="https://stackoverflow.com/questions/28953622/do-convolutional-neural-networks-suffer-from-the-vanishing-gradient">source_1</a></p>
         <h2 id="howautoencoderworks">How autoencoder works?</h2>
         <h2 id="howtoimplementregularizationindecisiontreerandomforest">How to implement regularization in decision tree, random forest?</h2>
         <h2 id="whyrandomcolumnselectionhelpsrandomforest">Why random column selection helps random forest?</h2>
         <p>Random forest algorithm comes under the <code>Bootstrap Algorithm</code> a.k.a <code>Bagging</code> category whose primary objective is to <code>reduce the variance</code> of an estimate by averaging many estimates.</p>
         <ul>
            <li>
               sample datasets without replacement
               <ul>
                  <li>In bootstarp algorithm <code>M</code> decision trees are trained on <code>M</code> datasets which are sampled <code>without replacement</code> from the original datasets. Hence each of the sampled data will have duplicate data as well as some missing data, which are present in the original data, as all the sampled datasets have same length. Thuse these bootstrap samples will have diversity among themselves, resulting the model to be diverse as well. </li>
               </ul>
            </li>
         </ul>
         <p>Along with the above methods <code>Random Forest</code> also does the following:</p>
         <ul>
            <li>
               Random subset of feature selection for building the tree
               <ul>
                  <li>This is known as subspace sampling</li>
                  <li>Increases the diversity in the ensemble method even more</li>
                  <li>Tree training time reduces.</li>
                  <li><strong>IMPORTANT:</strong> Also simply running the same model on different sampled data will produce highly correlated predictors, which limits the amount of variance reduction that is possible. So RF tries to <strong>decorrelate</strong> the base learners by learning trees based on a randomly chosen subset of input variables, as well as, randomly chosen subset of data cases.</li>
               </ul>
            </li>
         </ul>
         <p>source: 1. Kevin Murphy, 2. Peter Flach</p>
         <p><a href="https://www.quora.com/How-does-bagging-avoid-overfitting-in-Random-Forest-classification">ref</a></p>
         <h2 id="overfittinginrandomforest">Overfitting in Random Forest</h2>
         <ul>
            <li>
               <p>To avoid over-fitting in random forest, the main thing you need to do is optimize a tuning parameter that governs the number of features that are randomly chosen to grow each tree from the bootstrapped data. Typically, you do this via k-fold cross-validation, where k∈{5,10}, and choose the tuning parameter that minimizes test sample prediction error. In addition, growing a larger forest will improve predictive accuracy, although there are usually diminishing returns once you get up to several hundreds of trees.</p>
            </li>
            <li>
               <p>For decision trees there are two ways of handling overfitting: (a) don't grow the trees to their entirety (b) prune. The same applies to a forest of trees - don't grow them too much and prune.</p>
            </li>
         </ul>
         <p><a href="https://stackoverflow.com/questions/34997134/random-forest-tuning-tree-depth-and-number-of-trees/35012011#35012011">ref 1</a></p>
         <p><a href="https://stats.stackexchange.com/questions/111968/random-forest-how-to-handle-overfitting">ref 2</a></p>
         <h2 id="howtoavoidoverfitting">How to avoid Overfitting?</h2>
         <p>Overfitting means High Variance</p>
         <p>Steps to void overfitting:</p>
         <ul>
            <li>Cross Validation</li>
            <li>Train with more data</li>
            <li>Remove feature</li>
            <li>Early stopping</li>
            <li>Regularizations</li>
            <li>Ensembling</li>
         </ul>
         <p><a href="https://elitedatascience.com/overfitting-in-machine-learning">ref 1</a></p>
         <h2 id="howtoavoidunderfitting">How to avoid Underfitting?</h2>
         <ul>
            <li>Add more features</li>
            <li>Add more data</li>
            <li>Decrease the amount of regularizations</li>
         </ul>
         <p><a href="https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html">ref 1</a></p>
         <h2 id="expectationmaximizationemgaussianmixturemodel">Expectation Maximization (EM), Gaussian Mixture Model</h2>
         <p>Gaussian mixture models are a probabilistic model for representing normally distributed subpopulations within an overall population. Mixture models in general don't require knowing which subpopulation a data point belongs to, allowing the model to learn the subpopulations automatically. Since subpopulation assignment is not known, this constitutes a form of <code>unsupervised learning</code>.</p>
         <p>For example, in modeling human height data, height is typically modeled as a normal distribution for each gender with a mean of approximately 5'10" for males and 5'5" for females. Given only the height data and not the gender assignments for each data point, the distribution of all heights would follow the sum of two scaled (different variance) and shifted (different mean) normal distributions. A model making this assumption is an example of a Gaussian mixture model (GMM), though in general a GMM may have more than two components. Estimating the parameters of the individual normal distribution components is a canonical problem in modeling data with GMMs.</p>
         <p><img src="https://ds055uzetaobb.cloudfront.net/image_optimizer/d47c612ae8c3dc7f5aef1fc66458456f4eea4145.png" alt="image" /></p>
         <p>A Gaussian mixture model is parameterized by two types of values, the mixture component weights and the component means and variances/covariances. For a Gaussian mixture model with $K$ components, the $k^{\text{th}}$ component has a mean of $\mu<em>k$ and variance of $\sigma</em>k$ for the <code>univariate case</code> and a mean of $\vec{\mu}<em>k$ and covariance matrix of $\vec{\sigma}</em>k$ for the <code>multivariate case</code>. The mixture component weights are defined as $\phi<em>k$ for component $C</em>k$, with the constraint that $\sum<em>{i=1}^k \phi</em>i = 1$, so that the total probability distribution normalizes to $1$. If the component weights aren't learned, they can be viewed as an <code>a-priori</code> distribution over components such that . If they are instead learned, they are the <code>a-posteriori</code> estimates of the component probabilities given the data.</p>
         <p><strong>One-dimensional Model:</strong></p>
         <p>$$
            p(x) = \sum<em>{i=1}^K\phi</em>i \mathcal{N}(x \;|\; \mu<em>i, \sigma</em>i)$$
            $$
            \mathcal{N}(x \;|\; \mu<em>i, \sigma</em>i) = \frac{1}{\sigma<em>i\sqrt{2\pi}} \exp\left(-\frac{(x-\mu</em>i)^2}{2\sigma<em>i^2}\right)
            $$
            $$\sum</em>{i=1}^K\phi_i = 1$$
         </p>
         <p><strong>Multi-dimensional Model</strong></p>
         <p>$$
            p(\vec{x}) = \sum<em>{i=1}^K\phi</em>i \mathcal{N}(\vec{x} \;|\; \vec{\mu}<em>i, \Sigma</em>i)
            $$
            $$
            \mathcal{N}(\vec{x} \;|\; \vec{\mu}<em>i, \Sigma</em>i) = \frac{1}{\sqrt{(2\pi)^K|\Sigma<em>i|}} \exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu}</em>i)^\mathrm{T}{\Sigma<em>i}^{-1}(\vec{x}-\vec{\mu}</em>i)\right)
            $$
            $$\sum<em>{i=1}^K\phi</em>i = 1$$
         </p>
         <h3 id="learningthemodel">Learning the Model</h3>
         <p>If the number of components $K$ is known, expectation maximization is the technique most commonly used to estimate the mixture model's parameters. In frequentist probability theory, models are typically learned by using maximum likelihood estimation techniques, which seek to maximize the probability, or likelihood, of the observed data given the model parameters. Unfortunately, finding the maximum likelihood solution for mixture models by differentiating the log likelihood and solving for 0 is usually analytically impossible.</p>
         <p>Expectation maximization (EM) is a numerical technique for maximum likelihood estimation, and is usually used when closed form expressions for updating the model parameters can be calculated (which will be shown below). Expectation maximization is an iterative algorithm and has the convenient property that the maximum likelihood of the data strictly increases with each subsequent iteration, meaning it is guaranteed to approach a local maximum or saddle point.</p>
         <p><strong>Note:</strong> Read section <code>8.5.1</code> from book <code>Element of Statistical Learning</code> for an easy understanding.</p>
         <h4 id="twocomponentmixturemodel">Two Component Mixture Model:</h4>
         <p>Say we have $|Y|$ number of data-<em>points coming from 2 normal distribution, i.e. <code>mixture of 2 gaussian distribution</code>. Where $Y</em>1 \sim N(\mu<em>1, \sigma</em>1)$ and $Y<em>2 \sim N(\mu</em>2, \sigma<em>2)$ and $|Y| = |Y</em>1| + |Y<em>2|$. Our task is to figure out those 2 distribution. More formally we want to estimate $\hat\mu</em>1, \hat\sigma<em>1$ and $\hat\mu</em>2, \hat\sigma_2$. We want to model $Y$ as follows </p>
         <p>$$Y = (1-\Delta)Y<em>1 + \Delta Y</em>2$$</p>
         <p>where $\Delta \in {0,1}$ with $P(\Delta=1)=\pi$. </p>
         <blockquote>
            <p>For simplicity we can think that $|Y|$ number of blue and yellow balls are there, where number of blue balls $|Y<em>1|$ and number of yellow balls $|Y</em>2|$. Blue balls come from 1 normal distribution and yellow balls come from another normal distribution. Now given a ball we need to figure out whether the balls come from 1st Normal distribution (blue) or 2nd normal distribution. We can easily solve this using <code>K-Means</code> clustering. But K-Means is a <code>Hard-Clustering</code> problem. Where assignment probability is 0 or 1. But Mixture model is a <code>Soft Clustering</code> problem and an iterative process and the assignment probability is $\in [0,1]$. </p>
         </blockquote>
         <p>In the above notation we can think of $\pi$ as the mixing coefficient. We can think it as a <code>prior probability</code>.</p>
         <p>Let $\phi_\theta(x)$ denotes the normal density with parameters $\theta = (\mu, \sigma^2)$. Then the density of $Y$ is </p>
         <p>$$g<em>Y=(1-\pi)\phi</em>\theta^1(y) + \pi\phi_\theta^2(y)$$</p>
         <p>Now we wish to fit this model to data by maximum likelihood estimation. The parameters are $\theta = (\pi, \theta^1, \theta^2) = (\pi, \mu<em>1, \sigma</em>1^2, \mu<em>2, \sigma</em>2^2)$.</p>
         <p>The log likelihood based on all the training case is:</p>
         <p>$$l(\theta; Z) = \sum<em>{i=1}^{N}log[(1-\pi)\phi</em>\theta^1(y<em>i) + \pi\phi</em>\theta^2(y_i)]$$</p>
         <p>But direct maximization (by taking grad and set to 0) is difficult due to the sum inside log.</p>
         <p>So we apply Expectation Maximization (EM) algorithm. </p>
         <ul>
            <li>
               <p>s1: guess: $\pi, \theta<em>1, \theta</em>2$</p>
            </li>
            <li>
               <p>s2: While $(oldLogLikelihood - Likelihood) > tolerance$</p>
               <ul>
                  <li>
                     <p>s3: <strong>Expectaton Step:</strong> Find Posterior Probability (<code>Responsibility</code>)</p>
                     <p>$$\hat\gamma<em>i = p(\theta</em>2|y<em>i) = \frac{p(y</em>i | \theta<em>2)p(\theta</em>2)}{p(y<em>i | \theta</em>1)p(\theta<em>1) + p(y</em>i |\theta<em>2)p(\theta</em>2)}$$</p>
                  </li>
                  <li>
                     <p>Where $p(\theta<em>2) = \pi$, $p(\theta</em>1) = 1-\pi$</p>
                  </li>
                  <li>
                     <p>$p(y<em>i) = p(y</em>i | \theta<em>1)p(\theta</em>1) + p(y<em>i |\theta</em>2)p(\theta_2)$</p>
                  </li>
                  <li>
                     <p>$p(y<em>i|\theta</em>2) = N(\mu<em>2, \sigma</em>2)$ [use from s1 (1st time) and s4 (later)]</p>
                  </li>
                  <li>
                     <p>s4: <strong>Maximization Step:</strong> compute the weighted mean and variances</p>
                  </li>
               </ul>
               <p>$$\hat\mu<em>1 = \frac{\Sigma(1-\hat\gamma</em>i)y<em>i}{\Sigma{1-\hat\gamma</em>i}}$$
                  $$\hat\mu<em>2 = \frac{\Sigma(\hat\gamma</em>i)y<em>i}{\Sigma{\hat\gamma</em>i}}$$
                  $$\hat\sigma<em>1 = \frac{\Sigma (1-\hat\gamma</em>i) (y<em>i - \hat\mu</em>1)^2}{\Sigma(1-\hat\gamma<em>i)}$$
                  $$\hat\sigma</em>2 = \frac{\Sigma \hat\gamma<em>i (y</em>i - \hat\mu<em>2)^2}{\Sigma{\hat\gamma</em>i}}$$
                  $$\hat\pi = \Sigma \frac{\hat\gamma_i}{N}$$
               </p>
               <ul>
                  <li>s5: calculate loglikelihood:</li>
               </ul>
               <p>$$l(\theta; Z) = \sum<em>{i=1}^{N}log[(1-\hat\pi)\hat\phi</em>\theta^1(y<em>i) + \hat\pi\hat\phi</em>\theta^2(y_i)]$$</p>
            </li>
         </ul>
         <p>So we are trying to maximize the loglikelihood through this iterative approach. The above algorithm is a simple version for 1D data. If the data is 2D then instead of variance $\sigma^2$, we will have covariance matrix $\Sigma$ because of Multivariate Gaussian. But the basic procedure is like this.</p>
         <p>For more details, read the following references.</p>
         <p><a href="https://www.youtube.com/watch?v=REypj2sy_5U">YouTube</a>, 
            [Elements of Statistical Learning, section <code>8.5.1</code>], 
            <a href="http://www.blackarbs.com/blog/intro-to-expectation-maximization-k-means-gaussian-mixture-models-with-python-sklearn/3/20/2017">code<em>implementation</a>,
            <a href="https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html">EM</em>algo<em>python</em>code</a>,
            <a href="http://emmanuel-klinger.net/expectation-maximization-in-python.html">code<em>other</a>,
            [book</em>Simon<em>Prince</em>ch7], 
            <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html">imp<em>source</em>1</a>,
            <a href="https://brilliant.org/wiki/gaussian-mixture-model/">imp<em>source</em>2</a>
         </p>
         <h2 id="bayesianmachinelearning">Bayesian Machine Learning</h2>
         <p>Coursera - Bayesian Methods for Machine Learning</p>
         <h2 id="1storderoptimization2ndorderoptimization">1st order optimization, 2nd order optimization</h2>
         <h2 id="underfittingoverfittingbiasvariance">Underfitting, Overfitting, Bias, Variance</h2>
         <h2 id="someothertopic">Some other Topic</h2>
         <h3 id="findingsimilarsetsfrommillionsorbillionsdata">Finding similar sets from millions or billions data</h3>
         <p>Techniques:</p>
         <ol>
            <li>Shingling: converts documents, emails etc to set.</li>
            <li>Minhashing: Convert large sets to short signature, while preserving similarity</li>
            <li>Locality-Sensitive-Hashing: Focus on pair of signatures likely to be similar.</li>
         </ol>
         <h4 id="minhashing">Minhashing</h4>
      </article>
      <nav id="mainNav">
         <div class="nav-wrapper">
            <a href="#" class="brand-logo right">Content</a>
            <ul id="nav-mobile" class="left hide-on-med-and-down">
               <li><a href="#linearregression">Linear Regression</a></li>
               <li><a href="#linearmodels">Linear Models</a></li>
               <li><a href="#eigendecomposition">Eigen Decomposition</a></li>
            </ul>
         </div>
      </nav>
   </body>
</html>

