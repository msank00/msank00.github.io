---
layout: post
title:  "Blog 200: Deep Learning: Natural Language Processing"
date:   2019-07-28 00:11:31 +0530
categories: jekyll update
mathjax: true
---

# **Quick Refresher: Neural Network**

## **Deep Feed Forward Network**

- These models are called Feed Forward Network because the information flows from $x$, through the intermediate computations used to define $f$ and finally to the output $y$.
- When FFN are extended to include feedback loops, they are called recurrent neural network.
- The dimensionality of the hidden layer decides the width of the model.
- The strategy of deep learning is to learn $\phi$. In this approach, we have a model $y=f(x;\theta,w)=\phi(x;\theta)^Tx$. We now have $\theta$, that we use to learn $\phi$ from a broad class of functions and parameter $w$ that map from $\phi(x)$ to the desired output. This is an example of deep FNN and $\phi$ denotes the hidden layer.
-  This approach gives up the convexity of the training problem but the benefits out-weight the harm.
- FFN has introduced the concept of **hidden layer**, and this requires us to choose the **activation function** to compute the the hidden layer values.


## **Gradient-Based Learning**

- The biggest difference between linear models and neural network is that the non-linearity of neural network causes the most interesting loss function to become **non-convex**.
- Neural network models are trained by using iterative, gradient based optimizers, that merely drive the cost function to a very low value.
- Where as linear equation solvers used to train linear regression models or the convex optimization algorithms, with **global convergence guarantees** used to train logistic regression or SVMs.
- SGD applied to non-convex loss function has **no convergence guarantees**, and is sensitive to the initial parameter values.

## **Short Summary of CNN**

- The interest in CNN started with AlexNet in 2012 and it has grown exponentially ever since.
- The main advantage of CNN compared to its predecessors is that it automatically detects the important features without any human supervision.
- CNN is also computationally efficient. It uses special convolution and pooling operations and performs parameter sharing. This enables CNN models to run on any device, making them universally attractive.

### **Architecture**

- All CNN models follow a similar architecture, as shown in the figure below.

![image](https://miro.medium.com/max/700/1*uulvWMFJMidBfbH9tMVNTw@2x.png)

#### **Convolution**

- We perform a series `convolution` + `pooling operations`, followed by a number of fully connected layers. 

![image](https://miro.medium.com/max/700/1*cTEp-IvCCUYPTT0QpE3Gjg@2x.png)

![image](https://miro.medium.com/max/700/1*ghaknijNGolaA3DpjvDxfQ@2x.png)

- The green area where the convolution operation takes place is called the `receptive field`.

![image](https://miro.medium.com/max/700/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif)

- **NOTE:** We perform multiple convolutions on an input, each using a different filter and resulting in a distinct feature map. We then stack all these feature maps together and that becomes the final output of the convolution layer.

![image](https://miro.medium.com/max/700/1*45GSvnTvpHV0oiRr78dBiw@2x.png)

#### **Non-Linearity**

- We again pass the result of the convolution operation through `relu` or `Leaky-Relu` activation function. So the values in the final feature maps are not actually the sums, but the relu function applied to them.

#### **Stride and Padding**

- Stride specifies how much we move the convolution filter at each step. By default the value is 1
- We see that the size of the feature map is smaller than the input, because the convolution filter needs to be contained in the input. If we want to maintain the same dimensionality, we can use padding to surround the input with zeros.
  
![image](https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif)

- The gray area around the input is the padding. We either pad with zeros or the values on the edge.

#### **Pooling**

- After a convolution operation we usually perform pooling to `reduce the dimensionality`. This enables us to reduce the number of parameters, which both shortens the training time and combats overfitting. Pooling layers downsample each feature map independently, reducing the height and width, keeping the depth intact.

![image](https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png)

- In CNN architectures, pooling is typically performed with 2x2 windows, stride 2 and no padding. While convolution is done with 3x3 windows, stride 1 and with padding.

#### **Hyperparameters**

- Filter size: we typically use `3x3` filters, but `5x5` or `7x7` are also used depending on the application.
- Filter count: this is the most variable parameter, it’s a power of two anywhere between 32 and 1024. Using more filters results in a more powerful model, but we risk overfitting due to increased parameter count. 
- `Stride`: we keep it at the default value 1.
- `Padding`: we usually use padding.

#### **Fully Connected**
- After the convolution + pooling layers we add a couple of fully connected layers to wrap up the CNN architecture.
- Remember that the output of both convolution and pooling layers are 3D volumes, but a fully connected layer expects a 1D vector of numbers. So we flatten the output of the final pooling layer to a vector and that becomes the input to the fully connected layer. 
- 

#### **Training:**

- You do not fix the filter coefficients, rather you learn them over several iterations of training. The initialization can be random, or can be based on pre-trained model weights (such as those from the 'modelzoo' in github repos for popular models such as Alexnet, VGG, etc)
- Once you decide the filter size, we randomly initialize the weight of the filter and allow back propagation algorithm to learn weights automatically.


**References:**

- [TDS: Applied-deep-learning-part-4](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)
- [Adesh Pande](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html?source=post_page)

#### **Backpropagation In Convolutional Neural Networks**

- [Imp_Blog](https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/?source=post_page)


### **VGG Model**

Let’s now take a look at an example state-of-the art CNN model from 2014. VGG is a convolutional neural network from researchers at Oxford’s Visual Geometry Group, hence the name VGG. It was the runner up of the ImageNet classification challenge with `7.3%` error rate.

Among the best performing CNN models, VGG is remarkable for its simplicity. Let’s take a look at its architecture.

![image](https://miro.medium.com/max/700/1*U8uoGoZDs8nwzQE3tOhfkw@2x.png)

- It only uses 3x3 convolutions throughout the network. 
- **NOTE:** The two back to back `3x3` convolutions have the effective receptive field of a `single 5x5` convolution. And three stacked `3x3` convolutions have the receptive field of a single 7x7 one. Here’s the visualization of two stacked 3x3 convolutions resulting in 5x5.

![image](https://miro.medium.com/max/700/1*YpXrr8bN5XyqOlztKPHvDw@2x.png)

- Does that mean, if `n` `3x3` filters are used back to back that is equivalent to applying 1 filter of size `2n+1`

- Another advantage of stacking two convolutions instead of one is that we use **two relu operations**, and **more non-linearity** gives **more power** to the model.

- The number of filters increase as we go deeper into the network. The spatial size of the feature maps decrease since we do pooling, but the depth of the volumes increase as we use more filters.

#### **Visualizing Feature Maps**

- VGG convolutional layers are named as follows: blockX_convY. For example the second filter in the third convolution block is called block3_conv2.

![image](https://miro.medium.com/max/700/1*OuxhgVj1WDDfo5UO5GIhgA@2x.png)

>>As we go deeper into the network, the feature maps look less like the original image and more like an abstract representation of it. As you can see in block3_conv1 the cat is somewhat visible, but after that it becomes unrecognizable. The reason is that deeper feature maps encode high level concepts like “cat nose” or “dog ear” while lower level feature maps detect simple edges and shapes. That’s why deeper feature maps contain less information about the image and more about the class of the image. They still encode useful features, but they are less visually interpretable by us.

![image](https://miro.medium.com/max/700/1*A86wUjL-Z0SWDDI3slKqtg@2x.png)

>> The feature maps become sparser as we go deeper, meaning the filters detect less features. It makes sense because the filters in the first layers detect simple shapes, and every image contains those. But as we go deeper we start looking for more complex stuff like “dog tail” and they don’t appear in every image. That’s why in the first figure with 8 filters per layer, we see more of the feature maps as blank as we go deeper (block4_conv1 and block5_conv1)

- Remember that each filter acts as a detector for a particular feature. The input image we generate will contain a lot of these features.

#### **Filter and Featuremap Visualization**

![image](https://miro.medium.com/max/700/1*w41F9cu7vnvts1e06VoK0A@2x.png)

>> As we go deeper into the network, the filters build on top of each other, and learn to encode more complex patterns. For example filter 41 in block5_conv3 seems to be a bird detector. You can see multiple heads in different orientations, because the particular location of the bird in the image is not important, as long as it appears somewhere the filter will activate. That’s why the filter tries to detect the bird head in several positions by encoding it in multiple locations in the filter.

- [Very_Imp_Blog](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)

### **How does CNN work ? Explain the implementation details?**
+ [Adit Deshpande Blog](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)
+ [cs231n](http://cs231n.github.io/)

## **Explain the back propagation steps of Convolution Neural Network?**
+ [Andrej Karpathy YouTube](https://www.youtube.com/watch?v=i94OvYb6noo)
+ [Andrej Karpathy Medium](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
+ [summary short](https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199)
+ [how to compute backprop in program](http://cs231n.github.io/optimization-2/), **MUST read**



----

## **Short Summary of RNN, LSTM, GRU**

- RNN reads the document left to right and after every word updates the state. By the time it reaches the end of the word, the information obtained from the `first few` words, are completely lost. So we need some mechanism to retain these information.
- Now the most famous rchitecture is `LSTM` to address the above issues of RNN. The general idea is to implement 3 primary operations `selective read`, `selective write` and `selective forget`. i.e forget some old information, create new information and update the current information.
- All the variations of `LSTM` implemented these 3 operations with some tweak, like merging 2 operations into 1 and many more.

### **LSTM:**

- In the most popular version of LSTM, in each LSTM cell these 3 operations are implemented as 3 `Gate` 
  - `selective forget` implemented by `forget gate` $f_t$ 
  - `selective read` implemented by `input gate` $i_t$
  - `selective write` implemented by `output gate` $o_t$

- Each LSTM cell also has 2 states
  - $h_t$: output/hidden state (similar to hidden state of RNN)
  - $C_t$: Cell state or `running state` that acts as a memory containing a fraction of information from all the previous timestamps $t$

#### **Gates**

$$f_t = \sigma (W_f  h_{t-1} + U_f x_t + b_f)$$
$$i_t = \sigma (W_i  h_{t-1} + U_i x_t + b_i)$$
$$i_t = \sigma (W_o  h_{t-1} + U_o x_t + b_o)$$

$f_t$, $i_t$, $o_t$ decides the fraction of {$\in [0,1]$} forget, read and write respectively. 

#### **States**

$$\tilde{C_t} = \sigma(WH_{t-1} + Ux_t + b)$$
$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C_t}$$
$$h_t = o_t \odot \sigma(C_t)$$

Combining all together:

$$
 \begin{bmatrix} 
  i_t \\ 
  f_t \\
  o_t \\
  \tilde{C_t} 
\end{bmatrix}_{4x1}
 =
  \begin{bmatrix}
   \sigma  \\
   \sigma  \\
   \sigma  \\
   \sigma  
   \end{bmatrix}_{4x1} \odot (W_{4x2}
\begin{bmatrix}
   h_{t-1}  \\
   x_t  
   \end{bmatrix}_{2x1}
   + B_{4x1})
$$

$$
W = \begin{bmatrix} 
  W_i & U_i \\ 
  W_f & U_f\\
  W_o & U_o \\
  W_{\tilde{C_t}} & U_{\tilde{C_t}} 
\end{bmatrix}_{4x2},
B = \begin{bmatrix} 
  b_{ih} + b_{ix} \\ 
  b_{fh} + b_{fx}\\
  b_{oh} + b_{ox} \\
  b_{\tilde{C_t}h} + b_{\tilde{C_t}i} 
\end{bmatrix}_{4x1}
$$

Where $\tilde{C_t}$ is the new input or content which will be used to update the current cell state $C_t$ by forgetting a fraction $f_t$ of old cell state $C_{t-1}$ (`selective forget`) + adding fraction $i_t$ of new cell input $\tilde{C_t}$ (`selective read`). Finally $h_t$ is obtained by doing a `selective write` with the fraction $o_t$ and cell state $C_t$. Note $\odot$ is called the `pointwise product` or the `hadamard` product.


![lstm](/assets/images/lstm.png)
[image source, cs231n, lecture 10 page 96](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)

- For reference check the blog of Chris Olah and the lecture of Prof. Mitesh K. However, in Chris Olah's blog, he has concatenated $h_{t-1}$ and $x_t$, which leads to learning of fewer parameter.
- In Prof.Mitesh K's lecture, he has used notation $s_t$ for cell state. 

#### **Quick Summary**

![image](/assets/images/rnn_gru_lstm.png)

Reference:

- [IMP, Lecture 10, Standford cs231 page-96 ](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)
- [Chrish Olah LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Lecture 15 by Prof. Mitesh K](https://www.youtube.com/watch?v=9TFnjJkfqmA&feature=youtu.be)
- [Edward Chen](http://blog.echen.me/2017/05/30/exploring-lstms/)


## What is BPTT and it's issues with sequence learning ?

**Total Loss:** In sequence learning (RNN, LSTM, GRU etc.) the total loss is simply the sum of the loss over all the time steps.

- Take a pause and think that for non sequence learning, there is no concept of each time step and they have only single loss function.
- In normal backpropagation, you need to get the gradient w.r.t weight and bias using chain rule and done. 
- Whereas, in sequence learning due to the presence of time step, you need to get the `loss gradient` for each time step and then aggregate them.

So total loss for sequence learning is 

$$L(\theta) = \sum\limits‎_{t=1}^{T} L_t(\theta) $$

![image](/assets/images/bptt.png)


In the above image is a simple vanila RNN, where $t$ is from $1$ to $4$.  For back propagation we need to compute the graident w.r.t $W$, $U$, $V$. 

Now taking derivative w.r.t $V$ is straight forward.

$$\frac{\delta L(\theta)}{\delta V} = \sum\limits_{t=1}^{T} \frac{\delta L_t(\theta)}{\delta V}$$


Probelm arrives now while taking derivative w.r.t $W$.

$$\frac{\delta L(\theta)}{\delta W} = \sum\limits_{t=1}^{T} \frac{\delta L_t(\theta)}{\delta W}$$

However, while applying chain rule $L_4(\theta)$ is dependent on $s_4$, $s_4$ is dependent on $W$ and $s_3$, $s_3$ is dependent on $s_2$ and so on. So while taking derivative of $s_i$ w.r.t $W$, we can't take $s_{i-1}$ as constant. That's the problem in such `ordered network`

Therefore, in such network the total derivative $\frac{\delta s_4}{\delta W}$ has 2 parts.

- **Explicit:** Treating all other input as constant
- **Implicit:** Summing over all indirect paths from $s_4$ to $W$.

![image](/assets/images/bptt_1.png)

![image](/assets/images/bptt_2.png)


Resource:

- [Lecture 14 by Prof.Mitesh K](http://www.cse.iitm.ac.in/~miteshk/CS7015.html)
- [Youtube lecture by Prof. Mitesh K](https://www.youtube.com/watch?v=Xeb6OjnVn8g&feature=youtu.be)
- [Blog by Danny Britz from WINDML](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)


## Vanishing Gradient in CNN and RNN:

- When talking about RNN, the vanishing gradient problem refers to the `change of gradient in one RNN layer over different time steps` (because the repeated use of the recurrent weight matrix). 
- On the contrary, when talking about CNN, it is about the `gradient change over different layers`, and it is generally referred to as `“gradient decaying over layers”`. Just for your information, the recent IndRNN model addresses the vanishing and exploding gradient problems. It is much more robust than the traditional RNN.
- If you just keep on adding convolution layers to a CNN , after a point you will start facing vanishing gradient. You can experiment this using the vgg architecture. To avoid this problem and build deeper networks , most of the modern architectures uses `skip connections` like in resent , inception. These modern architectures go deeper to more than 150 layers.
- RNNs unfolded are deep in common application scenarios, thus prone to severer vanishing gradient problems. For example, when used in language modeling, `RNN depth can go as long as the longest sentence in the training corpus`. If the model is character-based, then the depth is even larger. CNNs in typical application scenarios are `shallower`, but still suffer from the same problem with gradients.
- Images typically have `hierarchy of scales`, so parts of the image which are far away from each other typically interact only at `higher order layers of the network`.
  - For RNN parts which are far far away from each other can huge influence. e.g. If you smoke when you are young, you may have cancer 40 years later. RNN should be able to make predictions based on very long term correlations.

**TL;DR:** Deeper networks are more prone to Vanishing Gradient problem. Now RNNs are (after unfolding) very deep. So they suffer more from Vanishing Gradient problem. Here the Vanishing Gradient problem occurs at the gradient change at the same RNN layer. But CNN by default is shallow network. Here if you stack multiple CNN layer, then vanishing gradient occurs at decaying of gradient at different layer.

[Quora](https://www.quora.com/Why-doesnt-CNN-suffer-from-the-vanishing-gradient-problems-of-RNN)

## What is Exploding Gradient Problem??

**TL;DR:** Exploding gradients are a problem where large error gradients accumulate and result in very large updates to neural network model weights during training. This has the effect of your model being unstable and unable to learn from your training data.

### What Are Exploding Gradients?

An error gradient is the direction and magnitude calculated during the training of a neural network that is used to update the network weights in the right direction and by the right amount.

In deep networks or recurrent neural networks, error gradients can accumulate during an update and result in very large gradients. These in turn result in large updates to the network weights, and in turn, an unstable network. At an extreme, the values of weights can become so large as to overflow and result in `NaN` values.

The explosion occurs through exponential growth by repeatedly multiplying gradients through the network layers that have values larger than $1.0$.

### How do You Know if You Have Exploding Gradients?

There are some subtle signs that you may be suffering from exploding gradients during the training of your network, such as:

    - The model is unable to get traction on your training data (e.g. poor loss).
    - The model is unstable, resulting in large changes in loss from update to update.
    - The model loss goes to NaN during training.

There are some less subtle signs that you can use to confirm that you have exploding gradients.

    - The model weights quickly become very large during training.
    - The model weights go to NaN values during training.
    - The error gradient values are consistently above 1.0 for each node and layer during training.

### how to fix Exploding or Vanisihng Gradient in sequence model?

- Tranucated BPTT, i.e, you will not look back more than $k$ timesteps where $k \lt t$
- Gradient Clipping

Resource:

- [Lecture 14 by Prof.Mitesh Khapra](http://www.cse.iitm.ac.in/~miteshk/CS7015.html)
- [Youtube video of the above lecture](https://www.youtube.com/watch?v=EB1SoyivHFU&feature=youtu.be)


### How to Fix Exploding Gradients?

1. Re-Design the Network Model

  - In recurrent neural networks, updating across fewer prior time steps during training, called `truncated Backpropagation` through time, may reduce the exploding gradient problem.

2.  Using LSTM 

  - Exploding gradients can be reduced by using the Long Short-Term Memory (LSTM) memory units and perhaps related gated-type neuron structures. Adopting LSTM memory units is a new best practice for recurrent neural networks for sequence prediction.

3. Use Gradient Clipping

- Exploding gradients can still occur in very deep Multilayer Perceptron networks with a large batch size and LSTMs with very long input sequence lengths.
- If exploding gradients are still occurring, you can check for and limit the size of gradients during the training of your network. This is called `gradient clipping`.

`clipping gradients if their norm exceeds a given threshold`

4. Use Weight Regularization

[Ref: MachinelearningMastery](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)

### How does LSTM help prevent the vanishing (and exploding) gradient problem in a recurrent neural network?

There are two factors that affect the magnitude of gradients - the weights and the activation functions (or more precisely, their derivatives) that the gradient passes through.

If either of these factors is smaller than 1, then the gradients may vanish in time; if larger than 1, then exploding might happen. For example, the $tanh$ derivative is $<1$ for all inputs except 0; sigmoid is even worse and is always $≤ 0.25$.

    In the recurrency of the LSTM the activation function is the identity function with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.


The effective weight of the recurrency is equal to the forget gate activation. So, if the forget gate is on (activation close to 1.0), then the gradient does not vanish. Since the forget gate activation is never >1.0, the gradient can't explode either.
So that's why LSTM is so good at learning long range dependencies.

<img src="https://qph.fs.quoracdn.net/main-qimg-4359968f2dd46aaa1cf862d60724b453" alt="drawing" width="300"/>

---

**Solution 1)** Use activation functions which have ‘good’ gradient values. Not ZERO over a reasonable amount of range Not that small, Not that big. e.g. `ReLu`. [reference](https://www.quora.com/What-is-the-vanishing-gradient-problem/answer/Ottokar-Tilk)

**Solution 2)** Use `gating(pass or block, or in other words, 1 or 0) function`, not activation function. And train the ‘combination’ of all those gates. Doing this, no matter how ‘deep’ your network is, or how ‘long’ the input sequence is, the network can remember those values, as long as those gates are all 1 along the path. <- `This is how LSTM/GRU did the job`.

---

The vanishing (and exploding) gradient problem is caused by the repeated use of the recurrent weight matrix in RNN. In LSTM, the recurrent weight matrix is replaced by the `identity function` in the **carousel** and controlled by the forget gate. So ignoring the forget gate (which can always be open), the repeated use of the identity function would not introduce the vanishing (and exploding) gradient.

The recent **IndRNN**(Building A Longer and Deeper RNN) model also addresses the gradient vanishing and exploding problem. It uses learnable recurrent weights but regulated in a way to avoid the gradient vanishing and exploding problem. Compared with LSTM, `it is much faster and can be stacked into deep models. Better performance is shown by IndRNN over the traditional RNN and LSTM`.

[IndRNN](https://arxiv.org/abs/1803.04831)

---

Given that f, the forget gate, is the rate at which you want the neural network to forget its past memory, the error signal is propagated perfectly to the previous time step. In many LSTM papers, this is referred to as the **linear carousel** that prevents the vanish of gradient through many time steps.

[Quora Answer](https://www.quora.com/How-does-LSTM-help-prevent-the-vanishing-and-exploding-gradient-problem-in-a-recurrent-neural-network)

### Why can RNNs with LSTM units also suffer from “exploding gradients”?

In the paper Sequence to [Sequence Learning with Neural Networks (by Ilya Sutskever, Oriol Vinyals, Quoc V. Le)](https://arxiv.org/abs/1409.3215), and, in that paper, section "3.4 Training details", it is stated `Although LSTMs tend to not suffer from the vanishing gradient problem, they can have exploding gradients.`

**TL;DR:** LSTM decouples cell state (typically denoted by `c`) and hidden layer/output (typically denoted by `h`), and only do additive updates to `c`, which makes memories in `c` more stable. Thus the gradient flows through `c` is kept and hard to vanish (therefore the overall gradient is hard to vanish). However, other paths may cause gradient explosion.

**Detailed Answer:** [StackExchange](https://stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients/339129#339129)

**More Detailed Answer:** [LSTM: A search space odyssey](https://arxiv.org/abs/1503.04069)

## Gating in Deep learning

**Gating meaning:**

Although it can be a complex process and involve multiple gates or regions of interest, the process of **gating** is simply `selecting an area on the scatter plot generated during the flow experiment that decides which cells you continue to analyze and which cells you don’t.` [reference](https://bitesizebio.com/21596/an-introduction-to-gating-in-flow-cytometry/)

**TL;DR:** the process by which a channel in a cell membrane opens or closes. [Dictionary Meaning](https://www.dictionary.com/browse/gating)

In LSTM, gating decides how much cell state information, from previous step, to flow in the next step or not. Thus gating function is different from the Activation function and helps in case of Vanishing Gradient problem.

[Gating and Depth in the Neural Network](https://towardsdatascience.com/gating-and-depth-in-neural-networks-b2c66ae74c45)

### Why using sigmoid and tanh as the activation functions in LSTM or RNN is not problematic but this is not the case in other neural nets?

Commonly, $sigmoid$ and $tanh$ activation functions are problematic (gradient vanishing) in RNN especially when the training algorithm $BPTT$ is utilized. In LSTM, sigmoid and tanh are used to build the $gates$. Because of these gates, the gradient vanishing problem does not exist.  

## What's the difference between “hidden” and “output” in PyTorch LSTM?

According to Pytorch documentation 

```py
"""
Outputs: output, (h_n, c_n)

- output (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.
- h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len
- c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len
"""
```

How to interpret it ?

output comprises all the hidden states in the last layer ("last" depth-wise, not time-wise). (h_n, c_n) comprises the hidden states after the last timestep, t = n, so you could potentially feed them into another LSTM.

![image](https://i.stack.imgur.com/SjnTl.png)

The batch dimension is not included.

- [source_stackOverflow](https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm)


## Understand forward and backward with simple Perceptron

- Check the [[blog](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)] and it's implementation and result in the following [[notebook](https://github.com/subhendukhatuya/deeplearning_postmortem/blob/master/Unfolding_Perceptron_Forward_backward.ipynb)]
- Give attention to the intermediate values in the blog and in the code side by side and check they are equal.


## Tell about different loss function, when to use them?

>> Importantly, the choice of loss function is directly related to the activation function used in the output layer of your neural network. These two design elements are connected.

>> The choice of cost function is tightly coupled with the choice of output unit. Most of the time, we simply use the cross-entropy between the data distribution and the model distribution. The choice of how to represent the output then determines the form of the cross-entropy function.

**Regression Problem**

A problem where you predict a real-value quantity.

Case 1:

- `Output Layer Configuration:` One node with a linear activation unit.
- `Loss Function:` Mean Squared Error (MSE).

$$\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^{2}$$

Case 2:

- `Output Layer Configuration:` One node with a linear activation unit.
- `Loss Function:` Mean Squared Logarithmic Error (MSLE)


$$\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\big(\log(y^{(i)}+1)-\log(\hat{y}^{(i)}+1)\big)^{2}$$


### When to use case 2 ?

Mean Squared Logarithmic Error (MSLE) loss function is a variant of MSE, which is defined as above.

- It is usually used when the `true values` and the `predicted values` are very big. And therefore, their difference are also very big. In that situation, you generally don't want to penalizes huge difference between true and predicted die to their high value range.

- One use case say `sales price` prediction. Here the true value range can be very `skewed` which affects the prediction as well. Here regular `MSE` will heavily penalizes the big differences. But in this scenario it's wrong because here by default the value ranges are very big i.e. skewed. In this scenario one good approach is take the `log(true_value)` and predict that, and by default you have to use the `MSLE`.
- Another usefulness of applying `log` is it fixes the skeness and brings it more close to normal distribution. 


**Binary Classification Problem**


- `Output Layer Configuration:` One node with a sigmoid activation unit.
- `Loss Function:` Cross-Entropy, also referred to as Logarithmic loss.


**Multi-Class Classification Problem**


- `Output Layer Configuration:` One node for each class using the softmax activation function.
- `Loss Function:` Cross-Entropy, also referred to as Logarithmic loss.

In binary classification, where the number of classes `M` equals 2, cross-entropy can be calculated as:

$$-{(y\log(p) + (1 - y)\log(1 - p))}$$

If $M\gt2$ (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.

$$-\sum_{c=1}^My_{o,c}\log(p_{o,c})$$

Summary

![image](/assets/images/loss_function.png)

[Image Source: From Prof. Mitesh Khapra, Lecture 4, [slide page 95]](https://www.cse.iitm.ac.in/~miteshk/CS7015.html)

References:

- [Source MachineLearningMastery](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)
- [Source: How_to_be_on_top_0p3_percent_kaggle_competiion](https://www.kaggle.com/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition)
- [Blog](https://isaacchanghau.github.io/post/loss_functions/)
- [Deep Learning Lecture 4 by Prof. Mitesh khapra]((https://www.cse.iitm.ac.in/~miteshk/CS7015.html))


## How does Attention Works ?


 >> The attention-mechanism looks at an input sequence and decides at each step which other parts of the sequence are important.

 When reading a text, you always focus on the word you read but at the same time your mind still holds the important keywords of the text in memory in order to provide context.

An attention-mechanism works similarly for a given sequence. For our example with the human Encoder and Decoder, imagine that instead of only writing down the translation of the sentence in the imaginary language, the Encoder also writes down keywords that are important to the semantics of the sentence, and gives them to the Decoder in addition to the regular translation. Those new keywords make the translation much easier for the Decoder because it knows what parts of the sentence are important and which key terms give the sentence context.

References:

- [Paper: Effective Approaches to Attention-based Neural Machine Translation](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)
- [Visualizing A Neural Machine Translation Model by Jay Alammar](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [Important Blog](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
- [Imp: Attention in NLP](https://medium.com/@joealato/attention-in-nlp-734c6fa9d983)
- [Imp: Attention and Memory in NLP](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)


## What is Transformer and it's pros and cons?

From the Author:

    In paper “Attention Is All You Need”, we introduce the Transformer, a novel neural network architecture based on a self-attention mechanism that we believe to be particularly well suited for language understanding.  

Natural Language Understanding (NLU):  language modeling, machine translation and question answering

- Transformer outperforms both recurrent and convolutional models on academic English to German and English to French translation benchmarks. 
- On top of higher translation quality, the Transformer requires less computation to train and is a much better fit for modern machine learning hardware, speeding up training by up to an order of magnitude.

The paper ‘Attention Is All You Need’ describes transformers and what is called a sequence-to-sequence architecture. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the sequence of words in a sentence, into another sequence.

![transformer](https://cdn-images-1.medium.com/max/800/1*BHzGVskWGS_3jEcYYi6miQ.png)

- One interesting point is, even if it's used for seq2seq generation, but there is no `recurrence` part inside the model like the  `vanila rnn` or `lstm`.
- So one slight but important part of the model is the positional encoding of the different words. Since we have no recurrent networks that can remember how sequences are fed into a model, we need to somehow give every word/part in our sequence a relative position since a sequence depends on the order of its elements. These positions are added to the embedded representation (n-dimensional vector) of each word. 

Pros:
- Faster learning. More GPU efficient unlike the `vanila rnn`

References:

- [Paper: Attention is all you need](https://arxiv.org/abs/1706.03762)
- [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
- [Important Blog](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
- [Important: The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/) 

## What is LSTM, give it's equation, pros and cons?

## How does `word2vec` work?

Reference:

- [The Illustrated Word2vec by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)
- [Intuition & Use-Cases of Embeddings in NLP & beyond, ,talk by Jay Alammar](https://www.infoq.com/presentations/nlp-word-embedding/)

## What is Transfer Learning, BERT, Elmo, UmlFit ?


Reference:

- [The Illustrated BERT, ELMo, and co. by Jay Alammar](https://jalammar.github.io/illustrated-bert/)


## What is model Perplexity?

- In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.  [[wiki](https://en.wikipedia.org/wiki/Perplexity)]

- In natural language processing, perplexity is a way of evaluating language models. A language model is a probability distribution over entire sentences or texts. 

Reference:
- [Blog: Towardsdatascience](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3)

## What is BLEU score?

- BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations.
- Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.
- BLEU uses a modified form of precision to compare a candidate translation against multiple reference translations. The metric modifies simple precision since machine translation systems have been known to generate more words than are in a reference text. 
- 
>> The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is.


Reference:

- [Blog by machinelearningmastery](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)
- [Paper: BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf)

## Tell about different optimizer Adam, Rmsprop and their pros and cons?

Typically when one sets their learning rate and trains the model, one would only wait for the learning rate to decrease over time and for the model to eventually converge.

However, as the gradient reaches a `plateau`, the training loss becomes harder to improve. Dauphin et al argue that the difficulty in minimizing the loss arises from `saddle points` rather than poor local minima.

![image](https://miro.medium.com/max/700/0*Q_ZjKKXa9mTShbpV.png)

A saddle point in the error surface. A saddle point is a point where derivatives of the function become zero but the point is not a local extremum on all axes.

- **Batch gradient descent**
  
$$\theta = \theta - \eta \cdot \nabla_\theta J( \theta)$$

Batch gradient descent also doesn't allow us to update our model online, i.e. with new examples on-the-fly.

- **Stochastic gradient descent**

$$\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})$$

  Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.

While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, on the one hand, enables it to jump to new and potentially better local minima.

- **Mini-batch gradient descent**: Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n
training examples. 

$$\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})$$

Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used.

- **Momentum:** SGD oscillates across the slopes of the `ravine` while only making hesitant progress along the bottom towards the local optimum.

![image](https://www.researchgate.net/profile/Giorgio_Roffo/publication/317277576/figure/fig6/AS:500357433434112@1496305916279/6-LEFT-shows-a-long-shallow-ravine-leading-to-the-optimum-and-steep-walls-on-the.png)

[Image Author: Giorgio Roffo](https://www.researchgate.net/profile/Giorgio_Roffo)

LEFT: shows a `long shallow ravine` leading to the optimum and steep walls on the sides. Standard SGD will tend to oscillate across the narrow ravine. RIGHT: Momentum is one method for pushing the objective more quickly along the shallow ravine. 

$$ 
v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\ 
\theta = \theta - v_t $$

The momentum term $\gamma$ is usually set to `0.9` or a similar value.

Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. $\gamma \lt 1$). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.


- **Nesterov accelerated gradient (NAG):** However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.
  - Nesterov accelerated gradient (NAG) [6] is a way to give our momentum term this kind of prescience.
  
  $$v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} ) \\ \theta = \theta - v_t$$

All previous methods use the same learning rate for each of the parameter. Now we want different learning rate for different parameters. Here we go:

- **Adagrad:** It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with `sparse data`.

$$g_{t, i} = \nabla_\theta J( \theta_{t, i} )$$
$$\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}$$

$G_{t} \in \mathbb{R}^{d \times d}$ here is a diagonal matrix where each diagonal element $i,i$ is the sum of the squares of the gradients w.r.t. $\theta_i$ up to time step $t$, while $\epsilon$ is a smoothing term that avoids division by zero.

As $G_t$ contains the sum of the squares of the past gradients w.r.t. to all parameters $\theta$ along its diagonal, we can now vectorize our implementation by performing a matrix-vector product $\odot$ between $G_t$ and $g_t$:

$$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}$$

One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0.01 and leave it at that.

**Adadelta:** Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.


- **RMSprop**: It is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.

- **Adam:** Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $v_t$
like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients $m_t$ and $v_t$ respectively as follows:

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ 
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 $$

As mt and vt are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small.

They counteract these biases by computing bias-corrected first and second moment estimates:

$$\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1} \\ 
\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}$$

They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule:

$$\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

<img src="https://user-images.githubusercontent.com/11681225/49325458-fc785480-f585-11e8-8d2a-9012d6024c6e.gif" width="500" height="400" />


<img src="https://i.stack.imgur.com/gjDzm.gif" width="500" height="400" />


**Reference:**

- [Sebastian Ruder](http://ruder.io/optimizing-gradient-descent/)


## Tell about different loss function and their pros and cons?

### REGRESSION LOSS

- **Mean Squared Error Loss**: The Mean Squared Error, or MSE, loss is the default loss to use for `regression problems`.
Mathematically, it is the preferred loss function under the inference framework of maximum likelihood _if the distribution of the target variable is Gaussian_.

- **Mean Squared Logarithmic Error Loss:** There may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error. Instead, you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short. It has the effect of relaxing the punishing effect of large differences in large predicted values.
- **Mean Absolute Error Loss:** On some regression problems, the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value. The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.


![image](/assets/images/loss_function.png)

[Image Source: From Prof. Mitesh Khapra, Lecture 4, [slide page 95]](https://www.cse.iitm.ac.in/~miteshk/CS7015.html)

### BINARY CLASSIFICATION LOSS

- **Cross Entropy Loss or Negative Loss Likelihood (NLL):** It is the default loss function to use for binary classification problems. It is intended for use with binary classification where the target values are in the set `{0, 1}`. Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.

$$CrossEntropyLoss = -(y_i \log (\hat y_i) + (1-y_i) \log (1-\hat y_i))$$

- **Hinge Loss or SVM Loss:** An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with **Support Vector Machine** (SVM) models. It is intended for use with binary classification where the target values are in the set {-1, 1}. The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.

$$HingeLoss = \Sigma_{j \neq y_i} max(0, s_j - s_{y_i}+1)$$

### Multi-Class Classification Loss Functions

- **Categorical Cross Entropy:** It is the default loss function to use for multi-class classification problems. In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer value. Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.

$$CategoricalCrossEntropyLoss = -y_c \log (\hat y_c)$$

, where `c` is the class.

- **Sparse Categorical Cross Entropy:** A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process. For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory. Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.
- **Kullback Leibler Divergence Loss:** Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution. A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution. As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.

**Reference:**
- [MMM](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)
- [Blog](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)

## Autoencoders


- [Applied-deep-learning-part-3-autoencoders](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)

-----


## Word embedding - Implementation

Natural language processing systems traditionally treat words as discrete atomic symbols, and therefore 'cat' may be represented as Id537 and 'dog' as Id143. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that the model can leverage very little of what it has learned about 'cats' when it is processing data about 'dogs'.

Word embeddings transform sparse vector representations of words into a dense, continuous vector space, enabling you to identify similarities between words and phrases — on a large scale — based on their context.

`Vector space models` (VSMs) represent (`embed`) words in a continuous vector space where semantically similar words are mapped to nearby points (`'are embedded nearby each other'`). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the `Distributional Hypothesis`, which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories: 

- `count-based methods` (e.g. Latent Semantic Analysis)
- `predictive methods` (e.g. neural probabilistic language models).

This distinction is elaborated in much more detail by Baroni et al., but in a nutshell: 

- Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word. 
- Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model).


Neural probabilistic language models are traditionally trained using the maximum likelihood (ML) principle to maximize the probability of the next word $w_t$ (for "target") given the previous words $h$ (for "history") in terms of a softmax function

$$
P(w_t | h) = \text{softmax} (\text{score} (w_t, h))
$$
$$
           = \frac{\exp \{ \text{score} (w_t, h) \} }
             {\sum_\text{Word w' in Vocab} \exp \{ \text{score} (w', h) \} }
$$


where $score(w_t,h)$ computes the compatibility of word $w_t$  with the context $h$ (a dot product is commonly used). We train this model by maximizing its log-likelihood on the training set, i.e. by maximizing


$$J_\text{ML} = \log P(w_t | h)$$
$$= \text{score} (w_t, h) -
     \log \left( \sum_\text{Word w' in Vocab} \exp \{ \text{score} (w', h) \} \right).
$$

This yields a `properly normalized probabilistic` model for language modeling. However this is very expensive, because we need to compute and normalize each probability using the score for all other $V$ words $w'$ in the current context $h$ , at every training step.

![image](https://www.tensorflow.org/images/softmax-nplm.png)

[source_1](https://www.tensorflow.org/tutorials/representation/word2vec)

## Word2vec = Implementation

Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors:

- Continuous Bag-of-Words model (CBOW)
- Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.). 

Algorithmically, these models are similar, except that CBOW predicts target words (e.g. 'mat') from a sliding window of a source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. This inversion might seem like an arbitrary choice, but statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets. We will focus on the skip-gram model in the rest of this tutorial.


For feature learning in word2vec we do not need a full probabilistic model. The CBOW and skip-gram models are instead trained using a binary classification objective (logistic regression) to discriminate the real target words $w_t$ from $k$ imaginary (noise) words $\tilde w$, in the same context. We illustrate this below for a CBOW model. For skip-gram the direction is simply inverted.


 Google's word2vec is one of the most widely used implementations due to its training speed and performance. Word2vec is a predictive model, which means that instead of utilizing word counts à la  latent Dirichlet allocation (LDA), it is trained to predict a target word from the context of its neighboring words. The model first encodes each word using one-hot-encoding, then feeds it into a hidden layer using a matrix of weights; the output of this process is the target word. The word embedding vectors are are actually the weights of this fitted model. To illustrate, here's a simple visual: 

<img src="https://www.datascience.com/hs-fs/hubfs/Resources/Articles/nn_embed.png?t=1532623979827&width=1414&height=694&name=nn_embed.png" alt="image" width="600"/>

<img src="https://www.tensorflow.org/images/nce-nplm.png" alt="image" width="400"/>

[source_1](https://www.tensorflow.org/tutorials/representation/word2vec)

[source_2](https://www.datascience.com/resources/notebooks/word-embeddings-in-python)

[source_3](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)

## LDA, Topic Modelling

Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts.

Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.

Topic Models are very useful for the purpose for document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example – New York Times are using topic models to boost their user – article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.

<img src="https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling1.png" alt="image" width="400"/>

You can try doing topic modelling using the following methods.

- Term Frequency and Inverse Document Frequency. 
- Do Non negative Matrix Factorization (NMF)
- LDA. 

NMF is supposed to be a lot faster than LDA, but LDAs supposed to be more accurate. Problem is LDA takes a long time, unless you’re using distributed computing.

LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.

LDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. LDA converts this Document-Term Matrix into two lower dimensional matrices – M1 and M2.
M1 is a document-topics matrix and M2 is a topic – terms matrix with dimensions ($N$,  $K$) and ($K$, $M$) respectively, where $N$ is the number of documents, $K$ is the number of topics and $M$ is the vocabulary size.

It Iterates through each word `w` for each document `d` and tries to adjust the current topic – word assignment with a new assignment. A new topic `k` is assigned to word `w` with a probability P which is a product of two probabilities p1 and p2.

For every topic, two probabilities p1 and p2 are calculated. 

- P1: $p(topic_t | doc_d)$ = the proportion of words in document d that are currently assigned to topic t.
- P2: $p(word_w | topic_t)$ = the proportion of assignments to topic t over all documents that come from this word w.

The current topic – word assignment is updated with a new topic with the probability, product of p1 and p2 . In this step, the model assumes that all the existing word – topic assignments except the current word are correct. This is essentially the probability that topic t generated word w, so it makes sense to adjust the current word’s topic with new probability.

After a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. This is the convergence point of LDA.



[AVB](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)

[EdwinChen](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)

[medium-PySparkImplementation](https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e)

## LSTM demystified

![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)


**Core Idea:**
The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.


![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)

The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. It has `forget gate`, `input gate` and `output gate`.

**Forget Gate:**

![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)

It controls how much old information (or old memory or state) you want to retain or not. $f_t\in(0,1)$ and thus it maintains a proportion of old memory.

**Input Gate:**

![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)

The next step is to decide what new information we’re going to store in the cell state. This has two parts. 

- First, a sigmoid layer called the “input gate layer” decides which values we’ll update. 
- Second, a tanh layer creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.


**Cell Update:**

![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)

We multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add $i_t*\tilde{C}_t$. This is the new candidate values, scaled by how much we decided to update each state value.

**Output Gate:**

![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)

Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. 

- First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. 
- Second, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.

[chris_olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[medium](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)

[a_karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)



### why the input/output gates are needed ?

[source_1](https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html)

## Adaboost objective function

## Vanishing Gradient Problem in general 

[source](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)

## What is vanishing gradient problem in RNN?
If you don’t already know, the vanishing gradient problem arises when,during backprop, the error signal used to train the network exponentially decreases the further you travel backwards in your network. The effect of this is that the layers closer to your input don’t get trained.

To understand why LSTMs help, we need to understand the problem with vanilla RNNs. In a vanilla RNN, the hidden vector and the output is computed as such:

$$h_t = tanh(W_Ix_t + W_Rh_{t-1})$$
$$y_t = W_Oh_t$$

![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)

To do backpropagation through time to train the RNN, we need to compute the gradient of $E$ with respect to $W_R$. The overall error gradient is equal to the sum of the error gradients at each time step. For step $t$, we can use the multivariate chain rule to derive the error gradient as:

$$
\frac{\partial E_t}{\partial W_R} = \sum^{t}_{i=0} \frac{\partial E_t}{\partial y_t}\frac{\partial y_t}{\partial h_t}\frac{\partial h_t}{\partial h_i}\frac{\partial h_i}{\partial W_R}
$$

Now everything here can be computed pretty easily except the term 
$\frac{\partial h_t}{\partial h_i}$, which needs another chain rule application to compute:

$$
\frac{\partial h_t}{\partial h_i} = \frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}...\frac{\partial h_{i+1}}{\partial h_i} = \prod^{t-1}_{k=i} \frac{\partial h_{k+1}}{\partial h_k}
$$


Now let us look at a single one of these terms by taking the derivative of $h_{k+1}$ with respect to $h_k$ (where $diag$ turns a vector into a diagonal matrix):

$$
\frac{\partial h_{k+1}}{\partial h_k} =  diag(f'(W_Ix_i + W_Rh_{i-1}))W_R
$$

Thus, if we want to backpropagate through $k$ timesteps, this gradient will be :

$$
\frac{\partial h_{k}}{\partial h_1} =  \prod_i^k diag(f'(W_Ix_i + W_Rh_{i-1}))W_R
$$

As shown in this paper, if the dominant eigenvalue of the matrix $W_R$
 is greater than 1, the gradient explodes. If it is less than 1, the gradient vanishes.2 The fact that this equation leads to either vanishing or exploding gradients should make intuitive sense. Note that the values of $f'(x)$ will always be less than 1. So if the magnitude of the values of $W_R$ are too small, then inevitably the derivative will go to 0. The repeated multiplications of values less than one would overpower the repeated multiplications of $W_R$
. On the contrary, make $W_R$ too big and the derivative will go to infinity since the exponentiation of $W_R$ will overpower the repeated multiplication of the values less than 1. In practice, the vanishing gradient is more common, so we will mostly focus on that.

The derivative $\frac{\partial h_{k}}{\partial h_1}$ is essentially telling us how much our hidden state at time $k$ will change when we change the hidden state at time 1 by a little bit. According to the above math, if the gradient vanishes it means the earlier hidden states have no real effect on the later hidden states, meaning no long term dependencies are learned! This can be formally proved, and has been in many papers, including the original LSTM paper.


[source_1](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html)

## How does LSTM solve vanishing gradient problem?

the vanishing gradient problem arises when, during backprop, the error signal used to train the network exponentially decreases the further you travel backwards in your network. The effect of this is that the layers closer to your input don’t get trained. In the case of RNNs (which can be unrolled and thought of as feed forward networks with shared weights) this means that you don’t keep track of any long term dependencies. This is kind of a bummer, since the whole point of an RNN is to keep track of long term dependencies. The situation is analogous to having a video chat application that can’t handle video chats!

**LSTM Equation Reference:** Quickly, here is a little review of the LSTM equations, with the biases left off 

- $f_t=\sigma(W_f[h_{t-1},x_t])$
- $i_t=\sigma(W_i[h_{t-1},x_t])$
- $o_t=\sigma(W_o[h_{t-1},x_t])$
- $\widetilde{C}_t=tanh(W_C[h_{t-1},x_t])$
- $C_t=f_tC_{t-1} + i_t\widetilde{C}_t$
- $h_t=o_ttanh(C_t)$

![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

As we can see above (previous question), the biggest culprit in causing our gradients to vanish is that dastardly recursive derivative we need to compute: 
$\frac{\partial h_t}{\partial h_i}$. If only this derivative was ‘well behaved’ (that is, it doesn’t go to 0 or infinity as we backpropagate through layers) then we could learn long term dependencies!


**The original LSTM solution:** The original motivation behind the LSTM was to make this recursive derivative have a constant value. If this is the case then our gradients would neither explode or vanish. How is this accomplished? As you may know, the LSTM introduces a separate cell state $C_t$. In the original 1997 LSTM, the value for $C_t$ depends on the previous value of the cell state and an update term weighted by the input gate value. 

$$C_t = C_{t-1} + i\widetilde{C}_t$$

This formulation doesn’t work well because the cell state tends to grow uncontrollably. In order to prevent this unbounded growth, a forget gate was added to scale the previous cell state, leading to the more modern formulation:

$$C_t = fC_{t-1} + i\widetilde{C}_t$$

**A common misconception:** Most explanations for why LSTMs solve the vanishing gradient state that under this update rule, the recursive derivative is equal to 1 (in the case of the original LSTM) or f
(in the case of the modern LSTM)3 and is thus well behaved! One thing that is often forgotten is that f, i, and $\widetilde{C}_t$
are all functions of $C_t$, and thus we must take them into consideration when calculating the gradient.

The reason for this misconception is pretty reasonable. In the original LSTM formulation in 1997, the recursive gradient actually was equal to 1. The reason for this is because, in order to enforce this constant error flow, the gradient calculation was truncated so as not to flow back to the input or candidate gates. So with respect to $C_{t-1}$ they could be treated as constants. In fact truncating the gradients in this way was done up till about 2005, until the publication of this paper by Alex Graves. Since most popular neural network frameworks now do auto differentiation, its likely that you are using the full LSTM gradient formulation too! So, does the above argument about why LSTMs solve the vanishing gradient change when using the full gradient? The answer is no, actually it remains mostly the same. It just gets a bit messier. 

**Looking at the full LSTM gradient:** To understand why nothing really changes when using the full gradient, we need to look at what happens to the recursive gradient when we take the full gradient. For the derivation check this [link](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html). 

*Note:* One important thing to note is that the values 
$f_t$, $o_t$, $i_t$, and $\widetilde{C}_t$ are things that the network learns to set (conditioned on the current input and hidden state). Thus, in this way the network learns to decide when to let the gradient vanish, and when to preserve it, by setting the gate values accordingly!

This might all seem magical, but it really is just the result of two main things:

- The additive update function for the cell state gives a derivative thats much more ‘well behaved’
- The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.

### There is still a chance of gradient vanishing, but the model would regulate its forget gate value to prevent that from happening?

Yes this is correct. If the model is dumb and always sets its forget gate to a low value, then the gradient will vanish. Since the forget gate value is a learnable function, we hope that it learns to regulate the forget gate in a way that improves task performance

### How does this prevent gradient explosion? From my understanding gradient clipping is applied to present that from happening.

This does not really help with gradient explosions, if your gradients are too high there is little that the LSTM gating functions can do for you. There are two standard methods for preventing exploding gradients: Hope and pray that you don't get them while training, or use gradient clipping.
The latter has a greater success rate, however I have had some success with the former, YMMV.


## How does CNN solve vanishing gradient problem?

RNNs unfolded are deep in common application scenarios, thus prone to severer vanishing gradient problems. CNNs in typical application scenarios are shallower, but still suffer from the same problem with gradients.

The most recommended approaches to overcome the vanishing gradient problem are:

- Layerwise pre-training
- Choice of the activation function

You may see that the state-of-the-art deep neural network for computer vision problem (like the ImageNet winners) have used convolutional layers as the first few layers of the their network, but it is not the key for solving the vanishing gradient. The key is usually training the network greedily layer by layer. Using convolutional layers have several other important benefits of course. Especially in vision problems when the input size is large (the pixels of an image), using convolutional layers for the first layers are recommended because they have fewer parameters than fully-connected layers and you don't end up with billions of parameters for the first layer (which will make your network prone to overfitting).

However, it has been shown (like this paper) for several tasks that using Rectified linear units alleviates the problem of vanishing gradients (as oppose to conventional sigmoid functions).

[source_1](https://stackoverflow.com/questions/28953622/do-convolutional-neural-networks-suffer-from-the-vanishing-gradient)

## How autoencoder works?

## What is deep learning ? Difference between machine learning & Deep learning ?

------
## **How can you cluster documents in unsupervised way?**
- [source](https://github.com/utkuozbulak/unsupervised-learning-document-clustering) 

## **How do you find the similar documents related to some query sentence/search?**

+ simplest apporach is to do tf-idf of both documents and query, and then measure cosine distance (i.e., dot product)
+ on top of that, if you use SVD/PCA/LSA on the tfidf matrix, it should further improve results. 
+ [link](https://www.r-bloggers.com/build-a-search-engine-in-20-minutes-or-less/)


## **Explain TF-IDF** ?

- [link](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)

## <span style="color:red">**What is word2vec? What is the cost function for skip-gram model(k-negative sampling) ?**</span>

+ [cs224-lecture](https://www.youtube.com/watch?v=ASn7ExxLZws&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=3)
+ [keras implementation](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/)
+ [AVB-Different word counting technique](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)

## So As per my experience, Tf-Idf fails in document classification/clustering ? How can you improve further ?

## **What are word2vec vectors?**



## How did you perform language identification from text sentence ? (As in my CV)
## How does you represent the symbolic chinese or japanese alphabets here ?
## **How can I design a chatbot ? (I had little idea but I tried answering it with intent and response tf-idf based similarity)**
+ [Adit Deshpande](https://adeshpande3.github.io/adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me)
 
## **Can I develop a chatbot with RNN providing a intent and response pair in input ?**
## Suppose I developed a chatbot with RNN/LSTMs on Reddit dataset. 
   It gives me 10 probable responses ? 
   How can I choose the best reply Or how can I eliminate others replies ?**

1. How do you perform text classification ?
2. How can you make sure to learn a context !! Well its not possible with TF-IDF ? 
	+ I told him about taking n-grams say n = 1, 2, 3, 4 and concatenating tf-idf of them to make a long count vector ?
Okay that is the baseline people start with ? What can you do more with machine learning ? 
(I tried suggesting LSTM with word2vec or 1D-CNN with word2vec for classification but 
 he wanted improvements in machine learning based methods :-|)
10. **How does neural networks learns non-linear shapes when it is made of linear nodes ? What makes it learn non-linear boundaries ?**
------
1. **What is the range of sigmoid function** ?
2. 
3. Text classification method. How will you do it ?
4. Explain Tf-Idf ? **What is the drawback of Tf-Idf** ? How do you overcome it ?
5. What are bigrams & Tri-grams ? Explain with example of Tf-Idf of bi-grams & trigrams with a text sentence.
6. **What is an application of word2vec** ? Example.
7. **How will you design a neural network ?** How about making it very deep ? Very basic questions on neural network.?
8.  <span style="color:red">**How does LSTM work ? How can it remember the context ?**</span>
  + **Must Watch:** CS231n by Karpathy in 2016 course and Justin in 2017 course.
9.  How did you perform language identification ? What were the  feature ?
10. How did you model classifiers like speech vs music and speech vs non-speech ?
11. How can deep neural network be applied in these speech analytics applications ?
