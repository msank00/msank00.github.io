---
layout: post
title:  "Blog 203: Deep Learning: Natural Language Processing QnA"
date:   2019-07-28 00:11:31 +0530
categories: jekyll update
mathjax: true
---

## **How do you find the similar documents related to some query sentence/search?**

+ Simplest apporach is to do tf-idf of both documents and query, and then measure cosine distance (i.e., dot product)
+ On top of that, if you use SVD/PCA/LSA on the tfidf matrix, it should further improve results. 

Source:

- [Blog1](https://www.r-bloggers.com/build-a-search-engine-in-20-minutes-or-less/)
- [Imp Blog2](http://searchivarius.org/blog/brief-overview-querysentence-similarity-functions)

## **Latent Semantic Analysis (LSA) for Text Classification Tutorial**

- [Blog](https://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)

## **Explain TF-IDF ? What is the drawback of Tf-Idf ? How do you overcome it ?**

### **Advantages:**
- Easy to compute
- You have some basic metric to extract the most descriptive terms in a document
- You can easily compute the similarity between 2 documents using it

### **Disadvantages:**
- TF-IDF is based on the bag-of-words (BoW) model, therefore it does not capture position in text, semantics, co-occurrences in different documents, etc.
- For this reason, TF-IDF is only useful as a lexical level feature
- Cannot capture semantics (e.g. as compared to topic models, word embeddings)


- [link](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)

## **What is word2vec? What is the cost function for skip-gram model(k-negative sampling)?**

+ [cs224-lecture](https://www.youtube.com/watch?v=ASn7ExxLZws&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=3)
+ [keras implementation](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/)
+ [AVB-Different word counting technique](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)

## So As per my experience, Tf-Idf fails in document classification/clustering ? How can you improve further ?

## **What are word2vec vectors?**

## **How can I design a chatbot ? (I had little idea but I tried answering it with intent and response tf-idf based similarity)**
+ [Adit Deshpande](https://adeshpande3.github.io/adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me)
 
## **Can I develop a chatbot with RNN providing a intent and response pair in input ?**
## Suppose I developed a chatbot with RNN/LSTMs on Reddit dataset. 
   It gives me 10 probable responses ? 
   How can I choose the best reply Or how can I eliminate others replies ?**

1. How do you perform text classification ?
2. How can you make sure to learn a context !! Well its not possible with TF-IDF ? 
	+ I told him about taking n-grams say n = 1, 2, 3, 4 and concatenating tf-idf of them to make a long count vector ?
Okay that is the baseline people start with ? What can you do more with machine learning ? 
(I tried suggesting LSTM with word2vec or 1D-CNN with word2vec for classification but 
 he wanted improvements in machine learning based methods :-|)
10. **How does neural networks learns non-linear shapes when it is made of linear nodes ? What makes it learn non-linear boundaries ?**
------
1. **What is the range of sigmoid function** ?
2. 
3. Text classification method. How will you do it ?
4. Explain Tf-Idf ? 
5. What are bigrams & Tri-grams ? Explain with example of Tf-Idf of bi-grams & trigrams with a text sentence.
6. **What is an application of word2vec** ? Example.
7. **How will you design a neural network ?** How about making it very deep ? Very basic questions on neural network.?
8.  <span style="color:red">**How does LSTM work ? How can it remember the context ?**</span>
  + **Must Watch:** CS231n by Karpathy in 2016 course and Justin in 2017 course.
9.  How did you perform language identification ? What were the  feature ?
10. How did you model classifiers like speech vs music and speech vs non-speech ?
11. How can deep neural network be applied in these speech analytics applications ?

----

## **Role-specific questions**

**Natural language processing**

1. What is part of speech (POS) tagging? What is the simplest approach to building a POS tagger that you can imagine?
2. How would you build a POS tagger from scratch given a corpus of annotated sentences? How would you deal with unknown words?
3. How would you train a model that identifies whether the word “Apple” in a sentence belongs to the fruit or the company?
4. How would you find all the occurrences of quoted text in a news article?
5. How would you build a system that auto corrects text that has been generated by a speech recognition system?
6. What is latent semantic indexing and where can it be applied?
7. How would you build a system to translate English text to Greek and vice-versa?
8. How would you build a system that automatically groups news articles by subject?
9. What are stop words? Describe an application in which stop words should be removed.
10. How would you design a model to predict whether a movie review was positive or negative?

**Related fields such as information theory, linguistics and information retrieval**

1. What is entropy? How would you estimate the entropy of the English language?
2. What is a regular grammar? Does this differ in power to a regular expression and if so, in what way?
3. What is the TF-IDF score of a word and in what context is this useful?
4. How does the PageRank algorithm work?
5. What is dependency parsing?
6. What are the difficulties in building and using an annotated corpus of text such as the Brown Corpus and what can be done to mitigate them?

**Tools and languages**

1. What tools for training NLP models (nltk, Apache OpenNLP, GATE, MALLET etc…) have you used?
2. Do you have any experience in building ontologies?
3. Are you familiar with WordNet or other related linguistic resources?
4. Do you speak any foreign languages?

----

***Question Source**

- [MLInterview](https://github.com/theainerd/MLInterview)
- [Data-Science-Interview-Resources](https://github.com/rbhatia46/Data-Science-Interview-Resources)
- [NoML](https://weifoo.gitbooks.io/noml/content/)
- 