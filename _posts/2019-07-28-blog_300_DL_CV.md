---
layout: post
title:  "Blog 300: Deep Learning: Computer Vision"
date:   2019-07-28 00:11:31 +0530
categories: jekyll update
mathjax: true
---

# Content

1. TOC
{:toc}
---

# Short Summary of CNN

- The interest in CNN started with AlexNet in 2012 and it has grown exponentially ever since.
- The main advantage of CNN compared to its predecessors is that it automatically detects the important features without any human supervision.
- CNN is also computationally efficient. It uses special convolution and pooling operations and performs parameter sharing. This enables CNN models to run on any device, making them universally attractive.

## Architecture

- All CNN models follow a similar architecture, as shown in the figure below.

![image](https://miro.medium.com/max/700/1*uulvWMFJMidBfbH9tMVNTw@2x.png)

## Convolution

- We perform a series `convolution` + `pooling operations`, followed by a number of fully connected layers. 

<center>
<img src="https://miro.medium.com/max/700/1*cTEp-IvCCUYPTT0QpE3Gjg@2x.png" height="250">
</center>


<center>
<img src="https://miro.medium.com/max/700/1*ghaknijNGolaA3DpjvDxfQ@2x.png" height="250">
</center>

- The green area where the convolution operation takes place is called the `receptive field`.


<center>
<img src="https://miro.medium.com/max/700/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif" height="250">
</center>

- **NOTE:** We perform multiple convolutions on an input, each using a different filter and resulting in a distinct feature map. We then stack all these feature maps together and that becomes the final output of the convolution layer.


<center>
<img src="https://miro.medium.com/max/700/1*45GSvnTvpHV0oiRr78dBiw@2x.png" height="150">
</center>


## Non-Linearity

- We again pass the result of the convolution operation through `relu` or `Leaky-Relu` activation function. So the values in the final feature maps are not actually the sums, but the relu function applied to them.

## Stride and Padding

- Stride specifies how much we move the convolution filter at each step. By default the value is 1
- We see that the size of the feature map is smaller than the input, because the convolution filter needs to be contained in the input. If we want to maintain the same dimensionality, we can use padding to surround the input with zeros.
  
<center>
<img src="https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif" height="250">
</center>


- The gray area around the input is the padding. We either pad with zeros or the values on the edge.

## Pooling

- After a convolution operation we usually perform pooling to `reduce the dimensionality`. This enables us to reduce the number of parameters, which both shortens the training time and combats overfitting. Pooling layers downsample each feature map independently, reducing the height and width, keeping the depth intact.

<center>
<img src="https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png" height="150">
</center>

- In CNN architectures, pooling is typically performed with 2x2 windows, stride 2 and no padding. While convolution is done with 3x3 windows, stride 1 and with padding.

## Hyperparameters

- Filter size: we typically use `3x3` filters, but `5x5` or `7x7` are also used depending on the application.
- Filter count: this is the most variable parameter, it’s a power of two anywhere between 32 and 1024. Using more filters results in a more powerful model, but we risk overfitting due to increased parameter count. 
- `Stride`: we keep it at the default value 1.
- `Padding`: we usually use padding.

## Fully Connected

- After the convolution + pooling layers we add a couple of fully connected layers to wrap up the CNN architecture.
- Remember that the output of both convolution and pooling layers are 3D volumes, but a fully connected layer expects a 1D vector of numbers. So we flatten the output of the final pooling layer to a vector and that becomes the input to the fully connected layer. 


## Training:

- You do not fix the filter coefficients, rather you learn them over several iterations of training. The initialization can be random, or can be based on pre-trained model weights (such as those from the 'modelzoo' in github repos for popular models such as Alexnet, VGG, etc)
- Once you decide the filter size, we randomly initialize the weight of the filter and allow back propagation algorithm to learn weights automatically.


## CNN Dimension Analysis:

The input dimensions are as follows

- $W_I = 227$
- $H_I = 227$
- $D_I = 3$
- The filter is of scale $F = 11$, i.e `11x11x3`, where 3 is the same depth as $D_I$ .
- We apply $96$ Filter operations, so therefore $K = 96$
- We do not take any padding ($P=0$)
- We choose a stride length of $S = 4$
- Thus, going by the above information, the output volume can be calculated as follows:
- $W_O = \frac{W_I - F + 2P}{S}+1=55$
- $H_O = \frac{H_I - F + 2P}{S}+1=55$
- $D_O=K=96$
- Thus, the output of the convolutional layer has the dimensions `55x55x96`.

<object data="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+11_+Understanding+the+input_output+dimensions.pdf" type="application/pdf" width="700px" height="700px">
    <embed src="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+11_+Understanding+the+input_output+dimensions.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+11_+Understanding+the+input_output+dimensions.pdf">Download PDF</a>.</p>
    </embed>
</object>



## Differences between Fully-connected DNNs and CNNs

- Sparse Connectivity
- Weight Sharing


<object data="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+12_+Sparse+Connectivity+and+Weight+Sharing.pdf" type="application/pdf" width="700px" height="700px">
    <embed src="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+12_+Sparse+Connectivity+and+Weight+Sharing.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+12_+Sparse+Connectivity+and+Weight+Sharing.pdf">Download PDF</a>.
        </p>
    </embed>
</object>

## How to understand the LeNet architecture?

![Image-LeNet](https://missinglink.ai/wp-content/uploads/2019/08/LeNet-5-1998.png)


<object data="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+14_+Our+First+Convolutional+Neural+Network+(CNN).pdf" type="application/pdf" width="700px" height="700px">
    <embed src="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+14_+Our+First+Convolutional+Neural+Network+(CNN).pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+14_+Our+First+Convolutional+Neural+Network+(CNN).pdf">Download PDF</a>.
        </p>
    </embed>
</object>



**Remember:**

- Say I have an image size `64x64x3` and filter size is `4x4`. When I will `convolve` this filter over the image, the filter will be applied on all the `3` channels of the image simultaneously, i.e across the depth of the image (in general volume). So image `64x64x3` and a filter `4x4x3` (i.e the initial filter `4x4` is expanded to the depth and becomes `4x4x3` ) is applied and a scalar value is obtained after all the matrix element wise multiplication and sum. Next we will stride i.e convolve across the height and width of the image (but not across the depth as the filter depth and image depth are same) and get the next scalar value. So after the convolution I will get a set of scalar values i.e neurons arranged over a 2D plane.
  - So, a 3D filter when applied on 3D volume (when both depths are same) gives a 2D plane of neurons i.e scalar.
  - So if K such filters are used there will be K such 2D planes

- Next we apply non-linearity over those values by applying some activation function
- But when applying **max-pooling**, the max-pooing matrix will not be expanded across the depth and will be applied independently on the all the 2D planes (stacked together) obtained after the convolution followed by activation. So after the max-pooling the depth of the volume will be same as the depth of the last convolution step. [check Lecture 14, of Convolution Neural Network, PadhAI, in guvi.in]

**References:**

- [TDS: Applied-deep-learning-part-4](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)
- [Adesh Pande](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html?source=post_page)

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Backpropagation In Convolutional Neural Networks

- [Imp_Blog](https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/?source=post_page)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# VGG Model

Let’s now take a look at an example state-of-the art CNN model from 2014. VGG is a convolutional neural network from researchers at Oxford’s Visual Geometry Group, hence the name VGG. It was the runner up of the ImageNet classification challenge with `7.3%` error rate.

Among the best performing CNN models, VGG is remarkable for its simplicity. Let’s take a look at its architecture.


<center>
<img src="https://miro.medium.com/max/700/1*U8uoGoZDs8nwzQE3tOhfkw@2x.png" height="350">
</center>

- It only uses 3x3 convolutions throughout the network. 
- **NOTE:** The two back to back `3x3` convolutions have the effective receptive field of a `single 5x5` convolution. And three stacked `3x3` convolutions have the receptive field of a single 7x7 one. Here’s the visualization of two stacked 3x3 convolutions resulting in 5x5.


<center>
<img src="https://miro.medium.com/max/700/1*YpXrr8bN5XyqOlztKPHvDw@2x.png" height="200">
</center>

- Does that mean, if `n` `3x3` filters are used back to back that is equivalent     to applying 1 filter of size `2n+1`

- Another advantage of stacking two convolutions instead of one is that we use **two relu operations**, and **more non-linearity** gives **more power** to the model.

- The number of filters increase as we go deeper into the network. The spatial size of the feature maps decrease since we do pooling, but the depth of the volumes increase as we use more filters.


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

## How to extract the features and feed to other model?

![image](https://www.researchgate.net/profile/Max_Ferguson/publication/322512435/figure/fig3/AS:697390994567179@1543282378794/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png)

In the above VGG diagram the last 3 gree boxes are the Fully Connected (FC) layers and they are doing the classification. And before that FC layers, everything else are doing feature selection. 

Now if you pay attention to the last (from left to right) red box (after the max pooling, size: `7x7x512`), that encodes all the features for that image. 

Now you can take that out, flatten it as a `1D` vector and can pass it to other architecture. 

### Where is this useful?

Say you are working on Image to Caption generation task. There task is to generate caption `given image` and previous captions. So in this Encoder-Decoder model, instead of feeding the ras image (pixels value as vector), we can first pass the Image through VGG16 and then extract that red box (refer above image), flatten it and pass that (encoded features of image) to the encoder model.


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

## Visualizing Feature Maps

- VGG convolutional layers are named as follows: blockX_convY. For example the second filter in the third convolution block is called block3_conv2.

![image](https://miro.medium.com/max/700/1*OuxhgVj1WDDfo5UO5GIhgA@2x.png)

>>As we go deeper into the network, the feature maps look less like the original image and more like an abstract representation of it. As you can see in block3_conv1 the cat is somewhat visible, but after that it becomes unrecognizable. The reason is that deeper feature maps encode high level concepts like “cat nose” or “dog ear” while lower level feature maps detect simple edges and shapes. That’s why deeper feature maps contain less information about the image and more about the class of the image. They still encode useful features, but they are less visually interpretable by us.

![image](https://miro.medium.com/max/700/1*A86wUjL-Z0SWDDI3slKqtg@2x.png)

>> The feature maps become sparser as we go deeper, meaning the filters detect less features. It makes sense because the filters in the first layers detect simple shapes, and every image contains those. But as we go deeper we start looking for more complex stuff like “dog tail” and they don’t appear in every image. That’s why in the first figure with 8 filters per layer, we see more of the feature maps as blank as we go deeper (block4_conv1 and block5_conv1)

- Remember that each filter acts as a detector for a particular feature. The input image we generate will contain a lot of these features.

## Filter and Featuremap Visualization

![image](https://miro.medium.com/max/700/1*w41F9cu7vnvts1e06VoK0A@2x.png)

>> As we go deeper into the network, the filters build on top of each other, and learn to encode more complex patterns. For example filter 41 in block5_conv3 seems to be a bird detector. You can see multiple heads in different orientations, because the particular location of the bird in the image is not important, as long as it appears somewhere the filter will activate. That’s why the filter tries to detect the bird head in several positions by encoding it in multiple locations in the filter.

**Reference:**

- [Very_Imp_Blog](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# How does CNN work ? Explain the implementation details?

+ [Adit Deshpande Blog](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)
+ [cs231n](http://cs231n.github.io/)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Explain the back propagation steps of Convolution Neural Network?


+ [Andrej Karpathy YouTube](https://www.youtube.com/watch?v=i94OvYb6noo)
+ [Andrej Karpathy Medium](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
+ [summary short](https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199)
+ [MUST READ: how to compute backprop in program](http://cs231n.github.io/optimization-2/), 


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Why layer normalization is required in Deep Learning?

**Covariate Shift:** Covariate shift refers to the change in the distribution of the input values to a learning algorithm. For instance, if the train and test sets come from entirely different sources (e.g. training images come from the web while test images are pictures taken on the iPhone), the distributions would differ. The reason covariance shift can be a problem is that the behavior of machine learning algorithms can change when the input distribution changes.

A neural network changes the weights of each layer over the course of training. This means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the **input distribution changes with each step**. This is problematic because it **forces each intermediate layer to continuously adapt** to its changing inputs.


The basic idea behind batch normalization is to limit covariate shift by normalizing the activations of each layer (transforming the inputs to be mean 0 and unit variance). This, supposedly, allows each layer to learn on a more stable distribution of inputs, and would thus accelerate the training of the network.

Batch normalization is one of the reasons why deep learning has made such outstanding progress in recent years. Batch normalization enables the use of higher learning rates, greatly accelerating the learning process. It also enabled the training of deep neural networks with sigmoid activations that were previously deemed too difficult to train due to the vanishing gradient problem. Based on its success, other normalization methods such as layer normalization and weight normalization have appeared and are also finding use within the field.

Though batch normalization is now a standard feature of any deep learning framework and can be used off the shelf, using it naively can lead to difficulties in practice. Despite the simplicity of batch normalization, the actual reason why it is effective is more complicated than meets the eye (Hint: It’s not as simple as just saying “covariate shift”). 

This series of posts attempts to provide a holistic understanding of normalization methods in deep learning. This post attempts to provide a better intuition into batch normalization – the first normalization method to gain widespread usage – and why it improves performance.
1. Intuition 1: Covariate Shift

The first intuition concerns “covariate shift” (which is in the title of the original batch normalization paper).

Covariate shift refers to the change in the distribution of the input values to a learning algorithm. This is a problem that is not unique to deep learning. For instance, if the train and test sets come from entirely different sources (e.g. training images come from the web while test images are pictures taken on the iPhone), the distributions would differ. The reason covariance shift can be a problem is that the behavior of machine learning algorithms can change when the input distribution changes.

In the context of deep learning, we are particularly concerned with the change in the distribution of the inputs to the inner nodes within a network. A neural network changes the weights of each layer over the course of training. This means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the input distribution changes with each step. This is problematic because it forces each intermediate layer to continuously adapt to its changing inputs.

The basic idea behind batch normalization is to limit covariate shift by normalizing the activations of each layer (transforming the inputs to be mean 0 and unit variance). This, supposedly, allows each layer to learn on a more stable distribution of inputs, and would thus accelerate the training of the network.

In practice, restricting the activations of each layer to be strictly $0$ `mean` and `unit variance` can **limit the expressive power** of the network. Therefore, in practice, batch normalization allows the network to learn parameters $\gamma$ and $\beta$ that can convert the mean and variance to any value that the network desires.

<center>
<img src="https://i0.wp.com/mlexplained.com/wp-content/uploads/2018/01/batchnorm_algorithm-1.png" 
alt="image"  height="250"/>
</center>

**Batch normalization** is a normalization method that normalizes activations in a network across the mini-batch. For each feature, batch normalization computes the mean and variance of that feature in the mini-batch. It then subtracts the mean and divides the feature by its mini-batch standard deviation.


According to the book Deep Learning by Ian Goodfellow, batch normalization can be understood from the perspective of high-order interactions.


- In a neural network, changing one weight affects subsequent layers, which then affect subsequent layers, and so on.
- This means changing one weight can make the activations fly all over the place in complex ways.
- This forces us to use lower learning rates to prevent the gradients from exploding, or – if we use sigmoid activations – to prevent the gradients from disappearing.
- Batch normalization to the rescue! **Batch normalization re-parameterizes the network to make optimization easier**.
- With batch normalization, we can `control the magnitude and mean of the activations independent of all other layers`.
- This means weights don’t fly all over the place (as long as we have sensible initial means and magnitudes), and we can optimize much easier.

**Alternatives:**

![](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/11/Screen-Shot-2018-11-28-at-4.56.06-PM.png?w=522&ssl=1)

- Layer Normalization

![](https://i1.wp.com/mlexplained.com/wp-content/uploads/2018/01/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2018-01-11-11.48.12.png?resize=768%2C448&ssl=1)


In batch normalization, the statistics are computed across the batch and are the same for each example in the batch. In contrast, in layer normalization, the statistics are computed across each feature and are independent of other examples.


**More Explanation:**

![image](https://gradientscience.org/images/batchnorm/landscapes.jpg)

The `smoothing effect` of BatchNorm makes the optimization landscape much easier to navigate, which could explain the faster convergence and robustness to hyper-parameters observed in practice.


**Reference:**

- [An-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning](https://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/)
- [An-overview-of-normalization-methods-in-deep-learning](https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/)
- [batchnorm](http://gradientscience.org/batchnorm/)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

## When should we apply batch-normalization?

The original paper instructs to put the batchnorm before activation layer but there is ongoing debate (look at the below github and reddit link). Many people showed emperically that by applying batchnorm after the activation function they got better result.

BN is better understood as a technique which **reduces second-order relationships between parameters of different layers** than a method to reduce covariate shift. Thus, the before/after distinction doesn't matter, and differences in performance could simply be because of other particular artifacts of the model.

Source: the deep learning book, 

**Reference:**

- [Batchnorm Before/After](https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md)
- [Batch_normalization_before_or_after_relu](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/)
- [Youtube-Ian Goodfellow](https://www.youtube.com/embed/Xogn6veSyxA?start=325&end=664&version=3)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Linear Classification

![image](/assets/images/CV/image_1_KNN_1.png)
![image](/assets/images/CV/image_1_KNN_2.png)

- **Q:** With N examples, how fast are training and prediction?
  - **Ans:** 
    - Train O(1),     
    - Predict O(N)
    - This is bad: we want classifiers that are fastat prediction; slow for training is ok


**Reference:**

- [cs231n_2019_lecture02](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture02.pdf)

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----


# Linear Classification - Loss Function, Optimization

![image](/assets/images/CV/image_2_LOSS_1.png)
![image](/assets/images/CV/image_2_LOSS_2.png)
![image](/assets/images/CV/image_2_LOSS_3.png)
![image](/assets/images/CV/image_2_LOSS_4.png)

- Suppose that we found a $W$ such that $L = 0$. Is this $W$ unique?
  - NO. $2W$ also has $L = 0$


![image](/assets/images/CV/image_2_LOSS_5.png)

**Why regularize?**

- Express preferences over weights
- Make the model simple so it works on test data
- Improve optimization by adding curvature

![image](/assets/images/CV/image_2_LOSS_6.png)
 

- Q1: What is the `min/max` possible loss $L_i$?
  - A: min $0$, max $\infin$

- Q2: At initialization all s (score) will be approximately equal; what is the loss?
  - A: $\log(numberOfClass)$, e.g $\log(10)$ ≈ $2.3$


**Reference:**

- [cs231n_2019_lecture03](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture03.pdf)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Siamese Network

Siamese networks are a special type of neural network architecture. Instead of a model learning to classify its inputs, the neural networks learns to differentiate between two inputs. It learns the similarity between them.

- [blog1](https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e)
- [blog:Siamese Networks: Algorithm, Applications And PyTorch Implementation](https://becominghuman.ai/siamese-networks-algorithm-applications-and-pytorch-implementation-4ffa3304c18)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Object Detection

To build a model that can detect and localize specific objects in images.

Learning both regression and classification together.

Regression: 4 bounding box co-ordinates
Classification: Object Label

<center>
<img src="https://raw.githubusercontent.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/master/img/baseball.gif" height="300">
</center>


## Bounding box

- `Ground Truth Bounding Box`: 4-dimensional array representing the rectangle which surrounds the ground truth object in the image (related to the dataset)
- `Anchor Box`: 4-dimensional array representing the rectangular patch of the input image the model looks at, to figure out which objects it contains (related to the model)
- `Anchor Box Activations or Predicted Bounding Box`: 4-dimensional array which should be as close as possible to the Ground Truth Bounding Box. I found the difference between Anchor Box and its Activations extremely confusing. In fact, they are the same thing. The Activations are nothing else that an updated version of the Anchor Box as the training phase proceeds. We keep them separated just because (in this specific implementation of SSD) we apply some position/size constraints to the Activations, based on the position/size of the original Anchor. The job of the Anchor Box is to fit the Ground Truth Bounding Box (and its class) as well as possible. Nothing else.

<center>
<img src="https://www.francescopochetti.com/wp-content/uploads/2019/01/CVBB-768x852.png" height="400">
</center>

<center>
<img src="https://www.francescopochetti.com/wp-content/uploads/2019/01/architecture-2-1024x576.png" height="300">
</center>


**Reference:**

- [PyTorch-Tutorial-to-Object-Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)
- [Important Blog](http://francescopochetti.com/fast-ai-dl2-lesson-9-single-shot-detection-detailed-walkthrough/)
- [Deep Object Detection](http://www.cs.toronto.edu/~kkundu/CSC2523DeepObjDet.pdf)

----

# What is RCNN?

RCNN: Region Based CNN for Object Detection



<center>
<img src="/assets/images/image_09_rcnn_1.png" height="300">
</center>


<center>
<img src="/assets/images/image_09_rcnn_2.png" height="300">
</center>


<center>
<img src="/assets/images/image_09_rcnn_3.png" height="300">
</center>


# What is SPP-net?


<center>
<img src="/assets/images/image_09_rcnn_4.png" height="300">
</center>

# What is Fast-RCNN?


<center>
<img src="/assets/images/image_09_rcnn_5.png" height="300">
</center>

- Fast test-time line SPP-net
- One network, trained in one stage
- Higher mean average precision than slow RCNN and SPP-net



<center>
<img src="/assets/images/image_09_rcnn_6.png" height="300">
</center>


**Reference:**

- [FAIR: Ross Girshick](http://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf)
- [Barkley: Ross Girshick](https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf)
- [cs231: Fei Fei Li](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture11.pdf)

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Object detection with RCNN

**Steps:**

![image](/assets/images/image_09_rcnn_0.png)

## Region Proposal

![image](/assets/images/image_09_rcnn_7.png)
![image](/assets/images/image_09_rcnn_8.png)

## Feature Extraction

![image](/assets/images/image_09_rcnn_9.png)

## Classification

![image](/assets/images/image_09_rcnn_10.png)


## Regression

![image](/assets/images/image_09_rcnn_11.png)

Similarly e calculate for $y$ coordinate.

**Reference:**

- Padhai, Prof. Mikesh, Lecture: Object Detection

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# List of Object Detection Algorithms and Repositories

<center>
<img src="https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67" height="200">
</center>

- [awesome-object-detection](https://github.com/amusi/awesome-object-detection)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# What is YOLO?

YOLO: You Only Look Once


<center>
<img src="https://blog.paperspace.com/content/images/2018/04/yolo-5.png" height="500">
</center>


**Reference**

- [how-to-implement-a-yolo-object-detector-in-pytorch](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>


----

# Exercise:

1. Face Recognition
2. Single Shot Detection (SSD)

----

<a href="#Top" style="color:#023628;background-color: #f7d06a;float: right;">Back to Top</a>