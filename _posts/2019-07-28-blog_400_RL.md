---
layout: post
title:  "Reinforcement Learning"
date:   2019-07-28 00:11:31 +0530
categories: jekyll update
mathjax: true
---

# Content

1. TOC
{:toc}
---

# Beginner's Guide:

- [Blog: A Beginner's Guide to Deep Reinforcement Learning](https://skymind.ai/wiki/deep-reinforcement-learning)
- [Blog:An introduction to Q-Learning: Reinforcement Learning](https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/)
- [Blog: Introduction to Reinforcement Learning](https://blog.algorithmia.com/introduction-to-reinforcement-learning/)
- [AV: A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python](https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/)

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Multi Arm bandit

The multi-armed bandit problem is a classic problem that well demonstrates the `exploration vs exploitation` dilemma. Imagine you are in a casino facing `multiple slot machines` and each is configured with an `unknown probability` of how likely you can get a `reward` at one play. The question is: **What is the best strategy to achieve highest long-term rewards?**

![image](/assets/images/bern_bandit.png)

A naive approach can be that you continue to playing with one machine for many many rounds so as to eventually estimate the “true” reward probability according to the law of large numbers. However, this is quite wasteful and surely does not guarantee the best long-term reward.

**Reference:**

- [Book](https://arxiv.org/pdf/1904.07272.pdf)
- [Youtube: Exploitation vs Exploration](https://www.youtube.com/watch?v=sGuiWX07sKw&feature=youtu.be)
- [Important Blog](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)
- [Regret Analysis of Stochastic and Non-stochastic Multi-armedBandit Problems](http://sbubeck.com/SurveyBCB12.pdf)
- [Blog](https://banditalgs.com/)
- [IITM: CS6046: Multi-armed bandits](https://www.cse.iitm.ac.in/~prashla/cs6046.html)
- [AV: Reinforcement Learning Guide: Solving the Multi-Armed Bandit Problem from Scratch in Python](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/)
- [The Multi-Armed Bandit Problem and Its Solutions](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# How can we design a systematic strategy that adapts to these stochastic rewards?


<center>
<img src="https://miro.medium.com/max/742/1*Tt8A6mP98ibBlrlFD5UJxg.png" height="200">
</center>

This is our goal for the multi-armed bandit problem, and having such a strategy would prove very useful in many real-world situations where one would like to select the “best” bandit out of a group of bandits i.e. A/B testing, line-up optimization, evaluating social media influence.

we approach the multi-armed bandit problem with a **classical reinforcement learning** technique of an **epsilon-greedy agent** with a learning framework of reward-average sampling to compute the **action-value** $Q(a)$ to help the agent improve its future action decisions for **long-term reward maximization**.

- [Python Code](https://github.com/ankonzoid/LearningX/tree/master/classical_RL/multiarmed_bandit)
- [code](http://knawade.me/rl/2019/02/28/Multi-Arm-Bandit-Problem.html)




**Reference:**

- [multi-armed-bandit](https://towardsdatascience.com/solving-the-multi-armed-bandit-problem-b72de40db97c)

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

---

# Bandit Problem

This tutorial intends to be an introduction to stochastic and adversarial multi-armed bandit algorithms and to survey some of the recent advances. In the multi-armed bandit problem, at each stage, an agent (or decision maker) chooses one action (or arm), and receives a reward from it. The agent aims at maximizing his rewards. Since he does not know the process generating the rewards, he needs to explore (try) the different actions and yet, exploit (concentrate its draws on) the seemingly most rewarding arms.

Let's say the Bandit had $K$ arms and $N$ rounds.

- if $K \lt N$: Small set of actions
- if $K \gt N$: Large set of actions

**Reference:**

- [Slide: Bandit with small set of actions](https://948da3d8-a-62cb3a1a-s-sites.googlegroups.com/site/banditstutorial/home/slides/Bandit_small.pdf?)
- [Slide: Bandit with large set of actions](https://948da3d8-a-62cb3a1a-s-sites.googlegroups.com/site/banditstutorial/home/slides/Bandit_large.pdf)
- [ICML 2011 Tutorial](https://sites.google.com/site/banditstutorial/)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>


----

# Application of Bandit Algorithm

- Clinical trials/dose discovery
- Recommendation systems (movies/news/etc)
- **Advert placement**
- A/B testing
- Network routing
- **Dynamic pricing (eg., for Amazon products)**
- Waiting problems (when to auto-logout your computer)
- Ranking (eg., for search)
- A component of game-playing algorithms (MCTS)
- Resource allocation
- A way of isolating one interesting part of reinforcement learning

**Reference:**

- [Slide 1](https://tor-lattimore.com/downloads/talks/2018/aaai/finite-armed-bandits.pdf)
- [Book](https://tor-lattimore.com/downloads/book/book.pdf)


<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Online Learning

> An important area with a rich literature and multiple connections with game theory and optimization that is increasingly influencing the theoretical and algorithmic advances in machine learning.

- The online scenario involves multiple rounds where **training and testing phases are intermixed**. 
- At each round, the learner receives an unlabeled training point, makes a prediction, receives the true label, and incurs a loss. 
- The objective in the on-line setting is to minimize the cumulative loss over all rounds or to **minimize the regret**, that is the difference of the cumulative loss incurred and that of the best expert **in hindsight** (i.e with experience). 
- Unlike the traditional ML settings, **no distributional assumption** is made in on-line learning. In fact, instances and their labels may be chosen adversarially within this scenario.
- These algorithms process one sample at a time with an update per iteration that is often **computationally cheap** and **easy to implement**.

```py
# No distributional assumption.
# Worst-case analysis (adversarial).
# Mixed training and test.
# Performance measure: mistake model, regret.
```

## Similar Problems

- Route selection (internet, traffic).
- Games (chess, backgammon).
- Stock value prediction.
- Decision making

## How Online learning is different from traditional machine learning?

Traditional machine learning (PAC learning) follows the key assumption that the distribution over data points is fixed over time, both for training and test points, and that points are sampled in an i.i.d. fashion. Under this assumption, the natural goal is to learn a hypothesis with a small expected loss or generalization error. 

In contrast, with on-line learning, no distributional assumption is made, and thus there is no notion of generalization. Instead, the performance of on-line learning algorithms is measured using a mistake model and the notion of regret.

## What is Regret?

**Definition:** The regret at time $T$ is the difference
between the loss incurred up to $T$ by the algorithm
and that of the best expert in hindsight:

$$
R_T = L_T - L_T^{min}
$$

for best regret minimization algorithms: $R_T \leq O(\sqrt{T \log N})$

_*expert in hindsight: means the expert learns with more experiment_


## Prediction with expert advice

In this setting, at the $t$ th round, in addition to receiving $x_t \in X,$ the algorithm also receives advice $y_{t,i} \in Y, i \in [N],$ from $N$ experts. Following  the general framework of on-line algorithms, it then makes a prediction, receives the true label and incurs a loss. 
After $T$ rounds, the algorithm has incurred a cumulative loss. The objective in this setting is to minimize the regret $R_T$, also called **external regret**, which compares the cumulative loss of the algorithm to that of the best expert in hindsight after $T$ rounds:

$$
R_T = \Sigma_{t=1}^T L(\hat {y_t}, y_t) - min_{i=1}^N \Sigma_{t=1}^T L(\hat {y_{t,i}}, y_t) 
$$

## Conclusion

On-line learning, regret minimization:
- rich branch of machine learning.
- connections with game theory.
- simple and minimal assumptions.
- algorithms easy to implement.
- scale to very large data sets.


**Reference:**

- [Using Online Learning Algorithms for Computational Advertising](http://www.aioptify.com/optimization.php?utm_campaign=contentpromotion+(http://www.aioptify.com/optimization.php)&utm_medium=cpm&utm_source=quora)
- Book: Foundation of Machine Learning by Mehryar Mohri 

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Counterfactual Regret Minimization


**Reference:**

- [Blog+Implementation](http://knawade.me/learning/2019/03/18/Counterfactual-Regret-Minimisation.html)
- [AI, Poker and Regret](https://hackernoon.com/artificial-intelligence-poker-and-regret-part-1-36c78d955720)
- [counterfactual-regret-minimization-for-poker-ai](https://int8.io/counterfactual-regret-minimization-for-poker-ai/)

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Reinforcement Learning

<center>
<img src="https://cdn-images-1.medium.com/max/800/0*jB9mEPwxa1VeTjV0.png", height="200">
</center>

- The training and testing phases are also **intermixed** in reinforcement learning. 
- To collect information, the learner actively **interacts with the environment** and in some cases affects the environment, and receives an immediate **reward** for each action. 
- The object of the learner is to **maximize his reward over a course of actions** and iterations with the environment. 
- However, no long-term reward feedback is provided by the environment, and the learner is faced with the **exploration versus exploitation dilemma**, since he must choose between exploring unknown actions to gain more information versus exploiting the information already collected.


**Reference:**

- [My Journey to Reinforcement Learning — Part 2: Multi-Armed Bandit Problem](https://towardsdatascience.com/my-journey-to-reinforcement-learning-part-2-multi-armed-bandit-problem-eefe1afab73c)
- Book: Foundation of Machine Learning by Mehryar Mohri 

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Click Through Rate



**Reference:**
- [Paper: Beyond the Click-Through Rate: Web Link Selection with Multi-level Feedback](https://www.ijcai.org/Proceedings/2018/0459.pdf)
- [optimise-ad-ctr-with-reinforcement-learning](https://dimensionless.in/how-to-optimise-ad-ctr-with-reinforcement-learning/)

<a href="#Top" style="color:#2F4F4F;background-color: #c8f7e4;float: right;">Content</a>

----

# Exercise:

1. What is PAC learning? [Foundations of Machine Learning by Mehryar Mohri]
2. What is Counterfactual **Regret** Minimization ([Source](https://int8.io/counterfactual-regret-minimization-for-poker-ai/))
3. How online learning algorithms are connected to game theory
4. What is game theory and min-max algorithm


----

<a href="#Top" style="color:#023628;background-color: #f7d06a;float: right;">Back to Top</a>